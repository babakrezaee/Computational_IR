[
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "",
    "text": "2.1 Non-Parametric Models\nIn this context, we introduce the \\(K\\)-Nearest Neighbors (\\(K\\)-NN) algorithm, a classic example of a non-parametric model. Unlike parametric models, \\(K\\)-NN does not assume a specific functional form between \\(X\\) and \\(Y\\). Instead, it makes predictions based on the similarity between data points, using the values of the \\(K\\) nearest observations to estimate the outcome. While \\(K\\)-NN does not provide insights into the strength or nature of the association between the outcome variable and the predictors, it is often an effective algorithm for prediction, especially when the true relationship between variables is complex or unknown.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Introduction: What is scientific about Political Science?\nAs part of Popperian and Lakatosian research paradigm, mainstream political science relies on empirical analysis to evaluate developed theoretical arguments. While some scholars rely on qualitative methods, others employ quantitative approaches to test their hypotheses. Of course, some scholars mix qualitative and quantitative methods in their empirical analysis, known as mixed-methods.\nIn your other classes, you learn about how to develop a theoretical argument, and probably some , about a political phenomenon. In this class, we learn how to quantitatively evaluate our developed theoretical argument. For Popper, a theory is only scientific if its hypotheses are empirically testable and falsifiable, that is, if it is possible to specify observation statements which would prove it is wrong. Therefore, the goal of empirical analysis is not to prove a theory but to reject it. Therefore, when we conduct an empirical analysis and reject the null hypothesis, we say we found empirical support for our hypotheses because we could not reject them. Publishing scientific research means you are inviting other scholars and scientific community in your field of study to scientifically attack your findings to reject them in their future studies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#correlation-causation-and-prediction",
    "href": "intro.html#correlation-causation-and-prediction",
    "title": "1  Introduction",
    "section": "1.1 Correlation, Causation, and Prediction",
    "text": "1.1 Correlation, Causation, and Prediction\nIn social science, we are interested in explaining whether and how two, sometimes more, factors/variables are associated. Generally, there are two groups of associations regarding the direction of associations: correlation vs. causation.\nSometimes our research is descriptive. Although nowadays, due to the causal inference revolution in economics and political science, correlation analysis is downplayed, these types of descriptive research play an important role, especially in introducing new topics to scientific fields. A new group of social scientists has started to explore how we can use machine learning algorithms to improve the prediction power of quantitative models. In this new paradigm, the main focus gives less weight to the causal association and tries to improve the explanatory power of models.\nIn a causal association, unlike a correlation analysis, the research design specifies an identification strategy to make sure that the direction of association is from the independent, also known as exogenous, variable toward the outcome, also known as endogenous, variable.\nFor example, think about the long-standing discussion about the association between economic development and democracy. It is accepted in the comparative political economy literature that there is a correlation between economic development and political development (democracy). However, within this literature, there has been a disagreement on the direction of the link between these two variables. Some scholars argue that economic development leads to political development, and some others contend that political development results in economic development.\nIf we want to show these different types of association schematically, they can be presented as follow:\n\nCorrelation\n\\[\nEconomic~development \\overset{?; +/-}{\\longleftrightarrow} Political~development\n\\]\nCausation\n\\[\nEconomic~development \\overset{+/-}{\\longrightarrow} Political~development\n\\]\n\\[\nEconomic~development \\overset{+/-}{\\longleftarrow} Political~development\n\\]\n\n\n1.1.1 The experimental ideal\nWhat does prevent us from distinguishing a correlation from causation in empirical research? There is a long list of issues from omitted variable problem and measurement problem to selection bias and reciprocal/bidirectional/simultaneous association that leads to biased estimation. We will, in detail, discuss what we mean by statistical estimation and bias later. However, roughly speaking, a biased estimation means that our estimated association using statistical models is significantly different from its true value.\nIn an ideal world, political scientists2 should have been able to use experimental labs to conduct a randomized experiment in a controlled environment to deal with the problems that cause biased estimations. In fact, most of the research in medical and hard sciences is laboratory research. Some social scientists also use lab and social experiments to test their hypotheses. Nevertheless, these scholars do not form a majority in political science. Also, using experimental design for answering some of social science questions is almost impossible.\nIn one of my recent research projects, I used an experimental design to explore whether providing information about the principles of nonviolent(civil) movements improves individuals’ attitudes toward this method of resistance. I defined a control group and a treatment group.3 Then, I assigned the participants in this experiment randomly to two groups. I showed a clip about the field of International Relations to the first group and a clip about the principles of nonviolent resistance to the second group. After showing the clips, I asked a set of questions and measured their attitudes toward nonviolent resistance. By calculating the average of attitudes in each group, I estimated the average difference between the attitude of treatment and control groups as a result of providing information about nonviolent resistance. Here, the randomization of assigning to treatment and control groups leads to a fundamental assumption: there is not a significant difference between the average attitude of participants in control and treatment groups before the start of experiment.4 This is the beauty of experimental design, although some scholars take further steps to make sure that the random assignment is indeed random by checking the balance over covariates!\n\n\n1.1.2 Observational data\nHowever, some scholars do not have the luxury of conducting experiments to test their hypotheses. Think about the scholars working on conflict! Can you study the effect of civil war on economic development using an experimental design?! Therefore, these scholars use another type of data that is known as observational data. This means scholars cannot manipulate the data generating process, and they sit/wait for a while for nature/society/world to generate the data for them, and then use these observed data to test their hypotheses. There is a rich literature on causal inference that explains how we can infer a causal association from observational data.\nHere, if we want to study the effect of variable/factor \\(x\\) on the outcome \\(y\\), we do not know whether the cases that we study them are assigned randomly to different values of \\(x\\) are or not! What does this mean? Remember the above example about the association between democracy and economic development. Can we claim that the variable democracy is assigned to countries randomly, so if we study its effect on the economy, we can see the pure effect of democracy on the economy?! Obviously, no! Some might argue that democratic countries are not democratic by chance, but they are democratic because of the quality of their economic institutions. Therefore, the high-level of economic development is not due to democracy but is the result of strong economic institutions such as property rights and free-market.\nThese types of problems, known as the selection problem, with observational data might be resolved by adding the omitted variable to the model. We later return to this omitted variable problem in our course. However, the selection problem, which is caused by our limited knowledge about the data generating process, is hard to address! Indeed, often more advanced methods, such as matching, Instrumental Variable, Simeltenous Equation Modeling, and regression discontinuity, are required to address the identification problems.\nDeveloping an identification strategy to conduct a causal inference is not within the scope of this course. That said, I will mention, and sometimes discuss in some details, how we can interpret our findings as causal inference. Instead, the main focus of this course is mainly to teach you how to use an unbiased and efficient statistical estimator to estimate the association between two variables. Also, you will learn how to infer about your theory from your statistical analysis, which is conducted in .\nThe purpose of the above discussion is that to give you a picture of what is one of the major challenges in quantitative analysis, and this course prepares you to take the initial steps toward learning about the foundations of these problems and how they can be possibly addressed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://www.britannica.com/science/scientific-hypothesis↩︎\nOf course, some political scientists do use experimental design. However, the share of these scholars varies across subfields.↩︎\nIt is called treatment because it comes from health and medical studies, where scientists used treatment for a disease.↩︎\nIf you enjoy math, here is the formalization: \\(E[Y_i|D_i=1]-E[Y_i|D_i=0]=E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=0]=E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=1]\\). The second part holds because in a randomized experiment, the pre-experiment value of the outcome variable for individual \\(i\\) is independent of allocation to the control and treatment groups! In other words, E(Y_{0i},D_i)=0↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational International Relations",
    "section": "",
    "text": "Preface\nThis handout is prepared for Computational International Relations by Dr. Babak RezaeeDaryakenari s.rezaeedaryakenari@fsw.leidenuniv.nl.\nThis is an ongoing project. Please do not share or use it without the author’s written permission.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "knn.html#leave-one-out-cross-validation",
    "href": "knn.html#leave-one-out-cross-validation",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "8.1 Leave-One-Out Cross-Validation",
    "text": "8.1 Leave-One-Out Cross-Validation\nIn this approach, we divide the sample into train and test samples. Assuming that the size of train sample is \\(n\\), we leave one observation out, say \\(i^{th}\\) observation, and train the model using \\(n-1\\) remaining observations. Then, we predict the outcome of \\(i^{th}\\) observation and compute the amount of prediction error: \\(RMSE_i=\\sqrt{(Y_i-\\hat{Y}_i)^2}\\). We repeat this procedure for all observations in the test sample and compute the mean of Cross-Validation RMSE:\n\\[\\begin{equation}\n\nRMSE_{LOCV}=\\frac{1}{n}\\sum_{i=1}^n RMSE_i\n\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#k-fold-cross-validation",
    "href": "knn.html#k-fold-cross-validation",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "8.2 \\(k\\)-fold Cross Validation",
    "text": "8.2 \\(k\\)-fold Cross Validation\nLeave-One-Out Cross-Validation is computationally intensive since you need to repeat the training and evaluation \\(n\\) times. An alternative approach is increasing the number of observations that are left out at each stage. To do so, we can divide the sample randomly to \\(k\\) almost equal size groups; then, we will leave one of these groups out for the validation and use the remaining \\(k-1\\) groups to train the model. Using this training model, we can predict the outcome for the observations in the validation subsample and then calculate the prediction for each of thoes observations. We will iterate the training and validation steps for each group (\\(k\\) times). Below figure shows a schematic view of \\(k\\)-fold Cross-Validation (KCV).\n\n\n\nThe photo is borrowed from: https://www.researchgate.net/figure/K-fold-cross-validation-E-is-the-overall-error-estimate_fig3_320270458\n\n\nConsidering the calculated error terms for each observation, we can compute the mean of \\(RMSE^{KCV}\\) as follow:\n\\[\\begin{equation}\n\nRMSE^{KCV}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2}\n\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nLets use the average of 5-,10-, and 15-fol \n\n\n\n\n\n\n\n\n\nThe best K is: 34",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#showing-variance-bias-tradeoff-by-an-example",
    "href": "knn.html#showing-variance-bias-tradeoff-by-an-example",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "8.3 Showing Variance-Bias-TradeOff by an Example",
    "text": "8.3 Showing Variance-Bias-TradeOff by an Example",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#how-does-k-nn-estimate-f.",
    "href": "knn.html#how-does-k-nn-estimate-f.",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.3 How does k-NN estimate f(.)?",
    "text": "2.3 How does k-NN estimate f(.)?\nWe first need to define a training dataset based on our main sample. The training set for a k-NN algorithm consist of the N pairs of the feature(x) and outcomes as follow:\n\\[\\begin{equation}\n\n\\{(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n) \\}\n\n\\end{equation}\\]\nWe start with a one feature model, but later in this handout, we will extend \\(k\\)-NN to the cases with more than one feature, i.e. \\(j&gt;1\\).\n\\(k\\)-NN calculates \\(f(x_i)\\) by taking the average of outcome values for the \\(k\\) nearest \\(x\\)’s to \\(x_i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#what-is-the-best-k",
    "href": "knn.html#what-is-the-best-k",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.4 What is the “best” K?",
    "text": "2.4 What is the “best” K?\nWe learned how K-NN works, but there is an issue here: K is chosen discretionary and for any \\(K\\), we can get (significantly) different results.\n\n\n\n\n\n\n\n\n\nIf we choose a large \\(k\\), then we get a simple line such that for k=500, the like is almost the mean of Median House price. On the other hand, choosing a small \\(k\\) gives a volatile estimation which captures the complexity of the data very well, yet it is very sensitive to any small changes in the data. Can we find a middle-way value for \\(k\\)? This is roughly the idea behind using cross-validation to overcome Bias-Variance Trade-Off. To discuss this idea more in detail, we need to learn about interpreting the prediction accuracy of an estimated function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#measuring-accuracy",
    "href": "knn.html#measuring-accuracy",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.5 Measuring Accuracy",
    "text": "2.5 Measuring Accuracy\nFor any given feature (\\(x\\in X\\)), or the set of features, we can find the predicted value of the outcome variable, \\(\\hat{y}\\), using the estimated function \\(\\hat{f}(x_i)\\). Given this information, we can find the prediction error for each observation by \\(y_i-\\hat{y}_i\\). The prediction error of each statistical model can be calculated using different indices based on these individual errors. One of the most common one is Root Mean Square Error (RMSE).\n\\[\\begin{equation}\n\nRMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n [y_i-\\hat{f}(x_i)]^2}\n\n\\end{equation}\\]\nRMSE calculates the average prediction errors.\nNow, let’s calculate the RMSE of predicting Median House Value in Boston Housing data when we use k-NN model, at different levels of k.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#bias-variance-tradeoff",
    "href": "knn.html#bias-variance-tradeoff",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.6 Bias-Variance Tradeoff",
    "text": "2.6 Bias-Variance Tradeoff\nBelow figure presents one of the core ideas of statistical learning. When the complexity of the model is low, training model has low variance but suffers high bias, which lead to a high prediction error. As the complexity of the model increases, the error of training sample decreases. However, since a model with a high level of complexity is trained to captured biases in the training sample, its prediction power for the test sample decreases, leading to a higher prediction error.\nWe can model association between \\(X\\) and \\(Y\\) as follow:\n\\[\\begin{equation}\n\n\nY= \\underbrace{f(X)}_\\text{Signal}+\\underbrace{\\epsilon}_{Noise}\n\n\\end{equation}\\]\n\\(\\epsilon\\) is the shock component, also known as noise, of the process. The goal of statistical modeling of a process is capturing the changes caused by \\(f(x)\\), as the shock/noise component is a random process. Thus, we do not want to mistakenly capture the changes caused by the noise part of the model as the changes caused by signal. If we fit a highly complex model which captures/model all changes in the outcome, then the risk of estimating a noisy model increases.\n\n\n\nTest and training error as a function of model complexity. (Copied from The Elements of Statistical Learning by Jerome, Friedman, Tibshirani, and Hastie)\n\n\nEstimating a simple model to explain a complex process leads to a bias function, thought this estimation has a low level of variance. On the other hand, a complex model is sensitive to every bit of changes in the data, which leads to a high variance, though the bias is low.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#cross-validation",
    "href": "knn.html#cross-validation",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.7 Cross-Validation",
    "text": "2.7 Cross-Validation\nDividing the sample to a train sample and a test sample allows us to evaluate the performance of predictive model. However, there are few challenges regarding this approach:\n\nHow can we be sure that the prediction results are independent of the train set? How can we draw the samples to address this concern?\nLeaving a part of dataset for validating the trained model means that we overlook part of information that we can get from the validation sub-sample. As we overlook this part of information to train our model, we might get a higher level of errors.\n\nThe suggested in statistical learning literature solution to address the above issues is Cross-Validation.\n\n2.7.1 Leave-One-Out Cross-Validation\nIn this approach, we divide the sample into train and test samples. Assuming that the size of train sample is \\(n\\), we leave one observation out, say \\(i^{th}\\) observation, and train the model using \\(n-1\\) remaining observations. Then, we predict the outcome of \\(i^{th}\\) observation and compute the amount of prediction error: \\(RMSE_i=\\sqrt{(Y_i-\\hat{Y}_i)^2}\\). We repeat this procedure for all observations in the test sample and compute the mean of Cross-Validation RMSE:\n\\[\\begin{equation}\n\nRMSE_{LOCV}=\\frac{1}{n}\\sum_{i=1}^n RMSE_i\n\n\\end{equation}\\]\n\n\n2.7.2 \\(k\\)-fold Cross Validation\nLeave-One-Out Cross-Validation is computationally intensive since you need to repeat the training and evaluation \\(n\\) times. An alternative approach is increasing the number of observations that are left out at each stage. To do so, we can divide the sample randomly to \\(k\\) almost equal size groups; then, we will leave one of these groups out for the validation and use the remaining \\(k-1\\) groups to train the model. Using this training model, we can predict the outcome for the observations in the validation subsample and then calculate the prediction for each of thoes observations. We will iterate the training and validation steps for each group (\\(k\\) times). Below figure shows a schematic view of \\(k\\)-fold Cross-Validation (KCV).\n\n\n\nThe photo is borrowed from: https://www.researchgate.net/figure/K-fold-cross-validation-E-is-the-overall-error-estimate_fig3_320270458\n\n\nConsidering the calculated error terms for each observation, we can compute the mean of \\(RMSE^{KCV}\\) as follow:\n\\[\\begin{equation}\n\nRMSE^{KCV}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2}\n\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nLets use the average of 5-,10-, and 15-fol \n\n\n\n\n\n\n\n\n\nThe best K is: 34 \n\n\n\n\n\n\n\n\n\n\n\n2.7.3 Showing Variance-Bias-TradeOff by an Example",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#k-nearest-neighbors-p1",
    "href": "knn.html#k-nearest-neighbors-p1",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.8 \\(k\\)-Nearest Neighbors, p>1",
    "text": "2.8 \\(k\\)-Nearest Neighbors, p&gt;1\nIn the previous sections, we studied the \\(kNN\\) prediction algorithm for the cases where there is only one feature. Finding the \\(k\\) nearest points for a problem with one feature/independent variable is straightforward, but how does the algorithm changes if we want to use more than one feature/independent variable?\nThe idea of \\(kNN\\) is predicting based on the most similar, closest, observations on the training subsample. Where \\(p&gt;1\\), the problem is finding the predicted values of \\(Y_k\\) given a set of the features/covariates \\(X_f=(x_{f1},x_{f2},\\dots,x_{fp})\\).\n\\(kNN\\):\nTo predict \\(\\hat{Y}_f\\) using \\(X_f=(x_{f1},x_{f2},\\dots,x_{fp})\\), we can define a neighborhood around \\(X_f\\) which include \\(k\\) nearest \\(X_i\\)’s to \\(X_f\\) on the training dataset; then, we calculate the average of \\(y_i\\)’s corresponding to these \\(k\\) smallest \\(d_i\\).\nThere are different distance metric. Here, we use one of the most well-known one, that is Euclidean Distance:\n\\[\\begin{equation}\n\nd_{if}=\\sqrt{\\sum_{j=1}^p (x_{fj}-x_{ij})^2}~, X_i~in~training~data\n\n\n\\end{equation}\\]\nThe second issue that we should address in employing \\(kNN\\) for problems with more than one feature is rescaling and normalizing the features.\nLet’s take a look at below scatter plots of Median Housing price versus Percentage of lower status of the population, lstat, and Weighted distances to five Boston employment centers, dis.\n\n\n\n\n\n\n\n\n\nAs you can see in the above photo, the unit of lstat is different from the unit of dis. This different between units of features affect finding the nearest points around \\(X_f\\). One common solution is rescaling/normalizing the features before applying \\(kNN\\) algorithm. There are two common rescaling approaches:\n\n\n\n\\[\\begin{equation}\n\n\\tilde{x}=\\frac{x-min(x)}{max(x)-min(x)}\n\n\\end{equation}\\]\nThis rescaling formula limit the X’s to \\(0\\) and \\(1\\).\n\n\n\n\\[\\begin{equation}\n\n\\tilde{x}=\\frac{x-\\bar{x}}{\\sigma_x}\n\n\\end{equation}\\]\nLet’s see how this rescaling changes the correlation between dis and medv:\n\n\n\n\n\n\n\n\n\nAs you can see in the above graphs, rescaling features does not change the association between the features and the outcome variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#a-prediction-problem-with-large-n-and-large-p-using-knn",
    "href": "knn.html#a-prediction-problem-with-large-n-and-large-p-using-knn",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.9 A Prediction Problem with Large \\(N\\) and Large \\(p\\) using \\(kNN\\)",
    "text": "2.9 A Prediction Problem with Large \\(N\\) and Large \\(p\\) using \\(kNN\\)\nNow, let’s predict the median price of houses in Boston data using several features. The features that I picked to predict the housing price, medv, are crim,indus,age,dis,black, and lstat. Before estimating the prediction model, it can be helpful to plot the scatter plots showing the association between the outcome and the features.\n\n\n\n\n\n\n\n\n\n\nlibrary(MASS)\n\nattach(Boston)\n\nlibrary(kknn)\nsource_url('https://raw.githubusercontent.com/babakrezaee/DataWrangling/master/Functions/docv_fun.R') ## Cross validation function\n\nX=cbind(crim,indus,age,dis,black,lstat)\n\nmmsc=function(x){return((x-min(x))/(max(x)-min(x)))}\n\nX_s=apply(X, 2, mmsc)\n\nset.seed(7)\n\nkv=2:100 #these are the k values (k as in kNN) we will try\n\nndocv=10 # Number of cross validation\n\n\n## 5-fold\ncv_mat_5=matrix(0,length(kv),ndocv)\n\n\n#pb &lt;- txtProgressBar(min = 0, max = ndocv, style = 3) # Setting up the progress bar\n\nfor (i in 1:ndocv){\n    cv_temp=docvknn(X_s,medv,kv,nfold=5)\n    cv_mat_5[,i]=sqrt(cv_temp/length(medv))\n    #setTxtProgressBar(pb, i)\n  }\ncv_mean_5=apply(cv_mat_5,1,mean)\n\nkbest_5 = kv[which.min(cv_mean_5)]\ncat(\"The min value of RMSE for 5-fold cross-validation is associated with k=\",kbest_5,\"\\n\")\n\nThe min value of RMSE for 5-fold cross-validation is associated with k= 7 \n\n## 10-fold\n\ncv_mat_10=matrix(0,length(kv),ndocv)\n\n\n#pb &lt;- txtProgressBar(min = 0, max = ndocv, style = 3) # Setting up the progress bar\nfor (i in 1:ndocv){\n      cv_temp=docvknn(X_s,medv,kv,nfold=10)\n      cv_mat_10[,i]=sqrt(cv_temp/length(medv))\n      #setTxtProgressBar(pb, i)\n}\n\ncv_mean_10=apply(cv_mat_10,1,mean)\n\nkbest_10 = kv[which.min(cv_mean_10)]\ncat(\"The min value of RMSE for 10-fold cross-validation is associated with k=\",kbest_10,\"\\n\")\n\nThe min value of RMSE for 10-fold cross-validation is associated with k= 5 \n\nplot(kv, cv_mean_5, xlab=\"K\", ylab=\"RMSE\", type='l', col=\"black\", lwd=2 )\nfor (i in 1:ndocv) lines(kv,cv_mat_5[,i], col=550+i, lwd=.4)\nlines(kv, cv_mean_5, xlab=\"K\", ylab=\"RMSE\", col=\"black\", lwd=2, lty=2 )\ntitle(main=\"nfold=5\", font.main= 1)\n\n\n\n\n\n\n\nplot(kv, cv_mean_10, xlab=\"K\", ylab=\"RMSE\", type='l', col=\"black\", lwd=2 )\nfor (i in 1:ndocv) lines(kv,cv_mat_10[,i], col=550+i, lwd=.4)\nlines(kv, cv_mean_10, xlab=\"K\", ylab=\"RMSE\", col=\"black\", lwd=2, lty=2 )\ntitle(main=\"nfold=10\", font.main= 1)\n\n\n\n\n\n\n\n##What is the best of way of measuring\n\ndata = data.frame(medv,X_s)\n\nkf_best_5 = kknn(medv~.,data,data,k=kbest_5,kernel = \"rectangular\")\nRMSE_fold_5=sqrt(sum((data$medv-kf_best_5$fitted)^2)/length(data$medv))\ncat(\"The RMSE for 5-fold kNN is\", RMSE_fold_5,\"\\n\")\n\nThe RMSE for 5-fold kNN is 4.01538 \n\nkf_best_10 = kknn(medv~.,data,data,k=kbest_10,kernel = \"rectangular\")\nRMSE_fold_10=sqrt(sum((data$medv-kf_best_10$fitted)^2)/length(data$medv))\ncat(\"The RMSE for 10-fold kNN is\", RMSE_fold_10,\"\\n\")\n\nThe RMSE for 10-fold kNN is 3.786824 \n\nlmf = lm(medv~.,data)\nfmat = cbind(medv,lmf$fitted,kf_best_5$fitted,kf_best_10$fitted)\n\nmy_line &lt;- function(x,y,...){\n    points(x,y,...)\n    segments(min(x), min(y), max(x), max(y),...)\n}\n\ncolnames(fmat)=c(\"y\",\"linear\", \"kNN5\", \"kNN10\")\npairs(fmat, lower.panel = my_line, upper.panel = my_line, col='maroon')\n\n\n\n\n\n\n\nprint(cor(fmat))\n\n               y    linear      kNN5     kNN10\ny      1.0000000 0.7715306 0.9015274 0.9118707\nlinear 0.7715306 1.0000000 0.8534194 0.8376299\nkNN5   0.9015274 0.8534194 1.0000000 0.9870313\nkNN10  0.9118707 0.8376299 0.9870313 1.0000000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html",
    "href": "NaiveBayes.html",
    "title": "3  Naive Bayes",
    "section": "",
    "text": "3.1 A Brief Introduction to Probability Theory\nIn social science, we are interested in studying the variations in one, sometime a set of, outcome across time and/or location: inter-state war, civil war, housing price, gender inequality, election results, and so forth. We often show this outcome variable, aka dependent variable, with \\(Y\\). Roughly speaking, a theory/hypothesis/proposition explain the association between the outcome variable \\(Y\\) with another random variable that is called explanatory variable, aka independent variable, exogenous variable, and feature, and is often shown by \\(X\\).\nVirtually, all statistical methods employed by social scientists aims to find and explain what is the probability of observing a specific outcome \\(Y=y\\) given a specific value of independent variable \\(X=x\\). We can write this as: \\(P(Y=y|X=x)\\). Some simply write this as \\(P(y|x)\\). As you will see in this course, and you might saw it in your other advanced methods course, the statistical techniques and algorithms that we use can be mainly divided to two groups based on whether \\(Y\\) is continuous or discrete/categorical. The latter one is often called classification question. (note: do not mix classification with clustering. These are two different types of problems. We will discuss clustering in the final sessions of this course.)\nThe common practice in studying \\(P(Y=y|X=x)\\) directly by making an assumption about the functional form of the association between \\(Y\\) and \\(X\\). For instance, in linear regression models such as OLS, we assume that the association between \\(Y\\) and \\(X\\) is linear: \\(y=\\alpha+\\beta x+\\epsilon\\). That said, you do not have to use probability models to develop a statistical learning model. You also can develop an algorithm and explore its performance. In fact, you can develop any algorithm you want, but you need to show that it works. \\(k\\) nearest neighbors (kNN) is one of these algorithms that does not need any probability model; we will cover kNN in this course. There is also another approach to study \\(P(Y=y|X=x)\\) using the Bayes Theorem, which is the topic of this session.\nWe will first briefly review very basic probability notations and concepts, then will learn the Bayes Theorem, and finally will study the Naive Bayes algorithm.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#conditional-joint-and-marginal-distributions",
    "href": "NaiveBayes.html#conditional-joint-and-marginal-distributions",
    "title": "3  Naive Bayes",
    "section": "3.2 Conditional, joint, and marginal distributions",
    "text": "3.2 Conditional, joint, and marginal distributions\n\nJoint (Probability) Distribution: Probability of observing two (or more) outcomes together is measured using joint probability distribution written as: \\(P(X=x, Y=y)\\). Of course, it can be extended to more than two outcomes:\n\n\\[P(X_1=x_1,X_2=x_2,\\dots,X_N=x_N)\\].\n\nConditional (Probability) Distribution: Intuitively, this means how knowing the values/outcomes of some variables affect our understanding about the values/outcomes of other variables. The simplest version of it for a two-variable case is P(Y=y|X=x), and the extended version of it is:\n\n\\[P(Y_1=y_1,Y=y_2,...,Y_L=y_L|X_1=x_1,X_2=x_2,\\dots,X_M=x_M).\\]\nSuppose that we are interested in modeling the type of contentious politics,\\(\\{0=Peace, 1=Nonviolent~Protests, 2=Violent~Protests, 3=Civil~War\\}\\), in The Republic of Gilead1 next year given the economic condition. For the sake of simplicity assume that there are two economic conditions: boom and bust.\n\n\n\n\n\n\n\n\n\n\\(S\\)\n\\(P(S=s\\|C=boom)\\)\n\\(S\\)\n\\(P(S=s\\|C=bust)\\)\n\n\n\n\nPeace\n\\(.80\\)\nPeace\n\\(.50\\)\n\n\nNonviolent Protests\n\\(.15\\)\nNonviolent Protests\n\\(.25\\)\n\n\nViolent Protests\n\\(.03\\)\nViolent Protests\n\\(.20\\)\n\n\nCivil war\n\\(.02\\)\nCivil war\n\\(.05\\)\n\n\n\nWe read P(S=Violent protests|E=bust)=.20: The probability/likelihood of “Violent protests” (S=Violent protests) conditional on/given a bad economy(C=bust) is \\(.20\\).\n\n\n\nOutcome tree\n\n\nThere are eight outcomes whose probabilities are conditional on the economic conditions:\n\n\n\n\\((C, S)\\)\n\\(P(S=s, C=c)\\)\n\n\n\n\n\\((Boom, Peace)\\)\n\\(.16\\)\n\n\n\\((Boom, Nonviolent)\\)\n\\(.07\\)\n\n\n\\((Boom, Violent)\\)\n\\(.006\\)\n\n\n\\((Boom, Civil~War)\\)\n\\(.004\\)\n\n\n\\((Bust, Peace)\\)\n\\(.4\\)\n\n\n\\((Bust, Nonviolent)\\)\n\\(.2\\)\n\n\n\\((Bust, Violent)\\)\n\\(.16\\)\n\n\n\\((Bust, Civil~War)\\)\n\\(.04\\)\n\n\n\n\n3.2.1 How are conditional, joint, and marginal probabilities related?\nJoint and conditional probabilities:\n\\[\\begin{align} \\label{standardization}\nP(Y=y,X=x)=P(X=x)P(Y=y|X=x)\\\\\n          =P(Y=y)P(X=x|Y=y)\n\\end{align}\\]\nJoint and marginal probabilities:\n\\[\\begin{align}\nP(X=x)=\\sum_y Pr(X=x,Y=y) \\\\\nP(Y=y)=\\sum_x Pr(X=x,Y=y)\n\\end{align}\\]\nFinding condtional probabilities from joints\nHere is the two-way probability table of above example.\n\n\n\nTwo-Way probability table\n\n\n\nWhat is the probability of civil war outcome?\nWhat is the probability of civil war outcome knowing that the economy is busting?\n\nSolution: we can formally write these two questions as\n\\(P(S=Civil~War)=?\\) and \\(P(S=Civil~War|E=Bust)=?\\)\n\\[\\begin{align}\nP(S=Civil~War) = \\sum_{e \\in E} P(S=Civil~War,E=e) \\\\\n               = P(S=Civil~War,E=Bust)\\\\\n               +P(S=Civil~War,E=Boost)\\\\\n               = .04+.004=.044\n\\end{align}\\]\nTo find \\(P(S=Civil~War|E=Bust)\\), we can use \\(P(Y=y,X=x)=P(X=x)P(Y=y|X=x)\\):\n\n\n\nTwo-Way probability table\n\n\n\\[\\begin{align}\nP(S=Civil~War|E=Bust) = \\frac{P(S=Civil~War,E=Bust)}{p(E=Bust)}\\\\\n                      =  \\frac{.04}{.8}=.05=5\\%\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#bayes-theorem",
    "href": "NaiveBayes.html#bayes-theorem",
    "title": "3  Naive Bayes",
    "section": "3.2 Bayes’ Theorem",
    "text": "3.2 Bayes’ Theorem\nWe are interested in finding \\(P(y|x)\\), and the common practice by frequentist statisticians is assuming a functional form for the association between \\(x\\) and \\(y\\), and then estimating the parameters of the function using different methods such as OLS, MLE, and so forth. However, there is another approach based on Bayes’ Theorem:\n\\[\\begin{align}\nP(y|x)=\\frac{P(x,y)}{p(x)}=\\frac{P(x,y)}{\\sum_y p(x,y)}=\\frac{p(y)p(x|y)}{\\sum_y p(y)p(x|y)}\n\\end{align}\\]\nIn other words, we can say Bayes’ Theorem suggest computing \\(P(y|x)\\) from \\(P(y)\\) and \\(P(x|y)\\).\nPicnic Day Example: Assume that you are planning to go out for a picnic today. However, you find that the weather is cloudy when you wake up. 50% of rainy days start off cloudy. However, cloudy days are common in your city. 40% of days start cloudy. Further, season it is a dry month, you know that in average 3 of 30 days end to rain, i.e. 10%. The question is what is the likelihood of raining today when it starts cloudy/\nSolution:\n\\[\\begin{align}\nP(Rain|Cloud)=\\frac{P(Cloud|Rain) P(Rain)}{P(Cloud)}\n             = \\frac{.1 \\times .5}{.4}=.125=12.5\\%\n\\end{align}\\]\nQuestion: What is the posterior odds ratio if \\(y={0,1}\\)?\n\n3.2.1 Extending Bayes’ Theorem to more than two variables\nOften, we face a question which requires modeling more than two variables. We still can extend the relationship between the joint and conditional distributions to more than two variables:\n\\[\\begin{align}\nP(Y_1=y_1, Y_2=y_2, Y_3=y_3)= P(Y_1=y_1) P(Y_2=y_2|Y_1=y_1) P(Y_3=y_3|Y_1=y_1, Y_2=y_2)\n\n\\end{align}\\]\n\n\n3.2.2 Independence\nConsider a set of random variables, \\(\\{x_1, x_2, \\dots, x_k\\}\\). They are independent of each other if the conditional distribution of any of them is not dependent on our observation of any others.\nExample: Suppose a fair coin is tossed 20 times. Let \\(Y_i\\) be 1 if the coin is a head at the \\(i^{th}\\) round, and \\(0\\) otherwise. What is \\(P(Y_{11}=1)\\)? What is \\(P(Y_{11}=1|y_1=1,\\dots,y_{10}=1)=?\\)\nIf two variables are independent, the the relation between the joint and conditional distributions changes to:\n\\[\\begin{align}\n\nP(Y=y|X=x)=P(Y=y)\n\n\\end{align}\\]\nSo,\n\\[\\begin{align}\n\nP(Y=y|X=x)=P(Y=y)\\\\\n\nP(X=x,Y=y)=P(Y=y|X=x) P(X=x)\\\\\n          =P(Y=y)P(X=x)\n\n\\end{align}\\]\nTheorem: \\(X\\) and \\(Y\\) are independent if and only if \\(P(x,y)=P(y|x)P(x)=p(y)p(x)\\)\n\n\n3.2.3 Identical distributions\nAnother common assumption in statistical analysis is that random variables are distributed identically. This means if we toss a coin 100 times, and \\(Y_i=1\\) and \\(0\\) otherwise, then \\(P(Y_{30}=1)=P(Y_{85}=1)\\)!\nWhen random variables are distributed identically and independently, they are called IID distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#extending-bayes-theorem-to-more-than-two-variables",
    "href": "NaiveBayes.html#extending-bayes-theorem-to-more-than-two-variables",
    "title": "3  Naive Bayes",
    "section": "3.3 Extending Bayes’ Theorem to more than two variables",
    "text": "3.3 Extending Bayes’ Theorem to more than two variables\nOften, we face a question which requires modeling more than two variables. We still can extend the relationship between the joint and conditional distributions to more than two variables:\n\\[\\begin{align}\nP(Y_1=y_1, Y_2=y_2, Y_3=y_3)= P(Y_1=y_1) P(Y_2=y_2|Y_1=y_1) P(Y_3=y_3|Y_1=y_1, Y_2=y_2)\n\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#independence",
    "href": "NaiveBayes.html#independence",
    "title": "3  Naive Bayes",
    "section": "3.4 Independence",
    "text": "3.4 Independence\nConsider a set of random variables, \\(\\{x_1, x_2, \\dots, x_k\\}\\). They are independent of each other if the conditional distribution of any of them is not dependent on our observation of any others.\nExample: Suppose a fair coin is tossed 20 times. Let \\(Y_i\\) be 1 if the coin is a head at the \\(i^{th}\\) round, and \\(0\\) otherwise. What is \\(P(Y_{11}=1)\\)? What is \\(P(Y_{11}=1|y_1=1,\\dots,y_{10}=1)=?\\)\nIf two variables are independent, the the relation between the joint and conditional distributions changes to:\n\\[\\begin{align}\n\nP(Y=y|X=x)=P(Y=y)\n\n\\end{align}\\]\nSo,\n\\[\\begin{align}\n\nP(Y=y|X=x)=P(Y=y)\\\\\n\nP(X=x,Y=y)=P(Y=y|X=x) P(X=x)\\\\\n          =P(Y=y)P(X=x)\n\n\\end{align}\\]\nTheorem: \\(X\\) and \\(Y\\) are independent if and only if \\(P(x,y)=P(y|x)P(x)=p(y)p(x)\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#identical-distributions",
    "href": "NaiveBayes.html#identical-distributions",
    "title": "3  Naive Bayes",
    "section": "3.5 Identical distributions",
    "text": "3.5 Identical distributions\nAnother common assumption in statistical analysis is that random variables are distributed identically. This means if we toss a coin 100 times, and \\(Y_i=1\\) and \\(0\\) otherwise, then \\(P(Y_{30}=1)=P(Y_{85}=1)\\)!\nWhen random variables are distributed identically and independently, they are called IID distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#next-week-spam-or-ham",
    "href": "NaiveBayes.html#next-week-spam-or-ham",
    "title": "3  Naive Bayes",
    "section": "4.1 Next week: Spam or Ham",
    "text": "4.1 Next week: Spam or Ham\nSentiment analysis is a process of understanding the options in a text using computational methods.\nEvery word in a piece of text can add information about the expressed opinion in it.\nHere, we do a simple analysis of texts to categorize them to Spam and Ham. In this example, we use the Bag of Words which does not consider the order of words, which contain information as well. However, surprisingly, it still a powerful too, as you see below.\nIn the Bag of Words method, these below texts are considered identical:\n\nSentence 1: There will be a presentation on Machine Learning in the school of management\nSentence 2: Learning will be a on Machine management in the school of There presentation.\n\nLet’s now do the coding part:\n\n## Loading the data of the web\nsmsData=read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/sms_spam.csv\", stringsAsFactors = FALSE, encoding=\"UTF-8\")\n\n\n## Let's take a look at the first few observations\n\nhead(smsData)\n\n  type\n1  ham\n2  ham\n3  ham\n4 spam\n5 spam\n6  ham\n                                                                                                                                                               text\n1                                                                                                                 Hope you are having a good week. Just checking in\n2                                                                                                                                           K..give back my thanks.\n3                                                                                                                       Am also doing in cbe only. But have to pay.\n4             complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\n5 okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm\n6                                                                                                                Aiya we discuss later lar... Pick u up at 4 is it?\n\nsummary(smsData$type)\n\n   Length     Class      Mode \n     5559 character character \n\nis.factor(smsData$type)\n\n[1] FALSE\n\n# We need to change the data type for \"type\" from text to factor\n\nsmsData$type=factor(smsData$type)\n\n# Let's look at the data now:\n\nsummary(smsData$type)\n\n ham spam \n4812  747 \n\nplot(smsData$type)\n\n\n\n\n\n\n\nis.factor(smsData$type)\n\n[1] TRUE\n\ndim(smsData)\n\n[1] 5559    2\n\nnames(smsData)\n\n[1] \"type\" \"text\"\n\nsmsData$type[1:5]\n\n[1] ham  ham  ham  spam spam\nLevels: ham spam\n\nsmsData$text[1:5]\n\n[1] \"Hope you are having a good week. Just checking in\"                                                                                                                \n[2] \"K..give back my thanks.\"                                                                                                                                          \n[3] \"Am also doing in cbe only. But have to pay.\"                                                                                                                      \n[4] \"complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\"            \n[5] \"okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm\"\n\n\nIn this question, the outcome variable is \\(y=type\\), and the feature is \\(x=text\\). We can check the frequency of words in the texts:\n\nlibrary(wordcloud)\n\n  wordcloud(smsData$text, min.freq = 1,\n          max.words=200, random.order=FALSE, rot.per=0, \n          colors=brewer.pal(8, \"Dark2\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#cleaning-up-the-data",
    "href": "NaiveBayes.html#cleaning-up-the-data",
    "title": "3  Naive Bayes",
    "section": "3.5 Cleaning up the data",
    "text": "3.5 Cleaning up the data\nWe need to prepare a corpus of texts, and the corpus needs some cleaning up.\n\nlibrary(tm)\nlibrary(SnowballC)\n\nsmsCorpus=VCorpus(VectorSource(smsData$text))\n\nsmsCl=tm_map(smsCorpus,content_transformer(tolower)) # Change all words from upper to lower letters\n\nsmsCl=tm_map(smsCl, removeNumbers)\nsmsCl=tm_map(smsCl, removeWords, stopwords())\nsmsCl=tm_map(smsCl, removePunctuation)\nsmsCl=tm_map(smsCl, stemDocument)\nsmsCl=tm_map(smsCl, stripWhitespace)\n\nNow, we can check how the data turned out:\n\nsmsData$text[1]\n\n[1] \"Hope you are having a good week. Just checking in\"\n\nsmsCl[[1]]$content\n\n[1] \"hope good week just check\"\n\n\nThe next step is creating a Document Term Matrix:\n\nsmsDTM=DocumentTermMatrix(smsCl)\n\ndim(smsDTM)\n\n[1] 5559 6559\n\nprint(as.matrix(smsDTM[100:110,100:110]))\n\n     Terms\nDocs  admiss admit ador adp adress adrian adrink adsens adult advanc adventur\n  100      0     0    0   0      0      0      0      0     0      0        0\n  101      0     0    0   0      0      0      0      0     0      0        0\n  102      0     0    0   0      0      0      0      0     0      0        0\n  103      0     0    0   0      0      0      0      0     0      0        0\n  104      0     0    0   0      0      0      0      0     0      0        0\n  105      0     0    0   0      0      0      0      0     0      0        0\n  106      0     0    0   0      0      0      0      0     0      0        0\n  107      0     0    0   0      0      0      0      0     0      0        0\n  108      0     0    0   0      0      0      0      0     0      0        0\n  109      0     0    0   0      0      0      0      0     0      0        0\n  110      0     0    0   0      0      0      0      0     0      0        0\n\n\nYou can check the frequent terms in the document as follow. In the example below, we want to find words that occur at least ten times:\n\nfindFreqTerms(smsDTM, lowfreq = 100)\n\n [1] \"ask\"    \"back\"   \"call\"   \"can\"    \"claim\"  \"come\"   \"day\"    \"dear\"  \n [9] \"dont\"   \"free\"   \"friend\" \"get\"    \"give\"   \"good\"   \"got\"    \"great\" \n[17] \"happi\"  \"hey\"    \"home\"   \"hope\"   \"just\"   \"know\"   \"later\"  \"like\"  \n[25] \"lor\"    \"love\"   \"make\"   \"meet\"   \"messag\" \"miss\"   \"mobil\"  \"much\"  \n[33] \"need\"   \"new\"    \"night\"  \"now\"    \"number\" \"one\"    \"phone\"  \"pleas\" \n[41] \"pls\"    \"repli\"  \"say\"    \"see\"    \"send\"   \"sorri\"  \"still\"  \"stop\"  \n[49] \"take\"   \"tell\"   \"text\"   \"thank\"  \"thing\"  \"think\"  \"time\"   \"today\" \n[57] \"tri\"    \"txt\"    \"wait\"   \"want\"   \"wat\"    \"way\"    \"week\"   \"well\"  \n[65] \"will\"   \"work\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#splitting-data-into-train-and-test",
    "href": "NaiveBayes.html#splitting-data-into-train-and-test",
    "title": "3  Naive Bayes",
    "section": "3.6 Splitting Data into Train and Test",
    "text": "3.6 Splitting Data into Train and Test\nWe divide the data to train and test sub-samples. We first train our algorithm using the train data, then we will evaluate its classifying power on the test data. We learned about cross-validation last session, but let’s for now do a simple one for the sake of training. There will be later assignments for you to explore how this can be done usng cross-vlidation method.\n\nsmsTrain=smsDTM[1:4000,]\nsmsTest=smsDTM[4001:5559,]\n\nsmsTrainY=smsData[1:4000,]$type\nsmsTestY=smsData[4001:5559,]$type\n\ncat(\"Training fraction is \",round((4000/5559)*100,2),\"% \\n\")\n\nTraining fraction is  71.96 % \n\n\nAs we saw above, there are many words that are repeated in the texts a few times. I will drop all words with frequency of less than 7 times over all texts.\n\nsmsWordsFreq=findFreqTerms(smsTrain,7) # Words with freq &gt;=7\n\nsmsFreqTrain=smsTrain[,smsWordsFreq]\nsmsFreqTest=smsTest[,smsWordsFreq]\n\n\nconvertCounts &lt;-function(x){\n\n  x&lt;-ifelse(x&gt;0, \"Yes\", \"No\")\n\n}\n\n# apply() convert_counts() to columns of train/test data\n\nsmsTrain=apply(smsFreqTrain, MARGIN = 2, convertCounts)\n\nsmsTest=apply(smsFreqTest,MARGIN = 2, convertCounts)\n\nNow, let’s check the final version of the prepared data:\n\ndim(smsTrain)\n\n[1] 4000  845\n\nis.matrix(smsTrain)\n\n[1] TRUE\n\nsmsTrain[10:12,1:10]\n\n    Terms\nDocs abl  abt  accept account across activ actual add  address admir\n  10 \"No\" \"No\" \"No\"   \"No\"    \"No\"   \"No\"  \"No\"   \"No\" \"No\"    \"No\" \n  11 \"No\" \"No\" \"No\"   \"No\"    \"No\"   \"No\"  \"No\"   \"No\" \"No\"    \"No\" \n  12 \"No\" \"No\" \"No\"   \"No\"    \"No\"   \"No\"  \"No\"   \"No\" \"No\"    \"No\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#footnotes",
    "href": "NaiveBayes.html#footnotes",
    "title": "3  Naive Bayes",
    "section": "",
    "text": "The Republic of Gilead is the fictional country name in the Handmaid’ tale Tale, Margaret Atwood, 1985.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#a-brief-introduction-to-probability-theory",
    "href": "NaiveBayes.html#a-brief-introduction-to-probability-theory",
    "title": "3  Naive Bayes",
    "section": "",
    "text": "3.1.1 Discrete Random Variables\nWe are uncertain about the value of a random variable. Thus, we use a (statistical) distribution to describe it. A discrete random variable can only take a countable number of values, and a probability distribution shows the probability of realization of each value/outcome. For example, if we toss a fair coin twice, then the distribution of the number of heads (X) can be written as follow:\n\n\n\n\\(X\\)\n\\(P(X=x)\\)\n\n\n\n\n\\(0\\)\n\\(.25\\)\n\n\n\\(1\\)\n\\(.5\\)\n\n\n\\(2\\)\n\\(.25\\)\n\n\n\nQuestion: What is \\(P(X \\geq 1)\\)? What is \\(P(X &gt; 1)\\)?\n\n\n3.1.2 Probability mass/denisty function and cumulative denisty function\n\\(f(x)=P(X=x)\\) is known as the probability mass function (PMF) when \\(X\\) is discrete, and the probability density function (PDF) when \\(X\\) is continuous. Also, the cumulative density function (CDF) is\n\\[F(x)=\\sum_{k\\leq x}f(k)~~if~~x \\in \\mathbb{Z}\\]\nand\n\\[F(x)=\\int_{k\\leq x}f(k)~~if~~x \\in \\mathbb{R}.\\]\nSome of the commonly used distributions are \\(X\\sim N(\\mu,\\sigma)\\), \\(X\\sim Bernoulli(p)\\), and \\(X\\sim Poisson(\\lambda)\\).\n\n\n3.1.3 Conditional, joint, and marginal distributions\n\nJoint (Probability) Distribution: Probability of observing two (or more) outcomes together is measured using joint probability distribution written as: \\(P(X=x, Y=y)\\). Of course, it can be extended to more than two outcomes:\n\n\\[P(X_1=x_1,X_2=x_2,\\dots,X_N=x_N)\\].\n\nConditional (Probability) Distribution: Intuitively, this means how knowing the values/outcomes of some variables affect our understanding about the values/outcomes of other variables. The simplest version of it for a two-variable case is P(Y=y|X=x), and the extended version of it is:\n\n\\[P(Y_1=y_1,Y=y_2,...,Y_L=y_L|X_1=x_1,X_2=x_2,\\dots,X_M=x_M).\\]\nSuppose that we are interested in modeling the type of contentious politics,\\(\\{0=Peace, 1=Nonviolent~Protests, 2=Violent~Protests, 3=Civil~War\\}\\), in The Republic of Gilead1 next year given the economic condition. For the sake of simplicity assume that there are two economic conditions: boom and bust.\n\n\n\n\n\n\n\n\n\n\\(S\\)\n\\(P(S=s\\|C=boom)\\)\n\\(S\\)\n\\(P(S=s\\|C=bust)\\)\n\n\n\n\nPeace\n\\(.80\\)\nPeace\n\\(.50\\)\n\n\nNonviolent Protests\n\\(.15\\)\nNonviolent Protests\n\\(.25\\)\n\n\nViolent Protests\n\\(.03\\)\nViolent Protests\n\\(.20\\)\n\n\nCivil war\n\\(.02\\)\nCivil war\n\\(.05\\)\n\n\n\nWe read P(S=Violent protests|E=bust)=.20: The probability/likelihood of “Violent protests” (S=Violent protests) conditional on/given a bad economy(C=bust) is \\(.20\\).\n\n\n\nOutcome tree\n\n\nThere are eight outcomes whose probabilities are conditional on the economic conditions:\n\n\n\n\\((C, S)\\)\n\\(P(S=s, C=c)\\)\n\n\n\n\n\\((Boom, Peace)\\)\n\\(.16\\)\n\n\n\\((Boom, Nonviolent)\\)\n\\(.07\\)\n\n\n\\((Boom, Violent)\\)\n\\(.006\\)\n\n\n\\((Boom, Civil~War)\\)\n\\(.004\\)\n\n\n\\((Bust, Peace)\\)\n\\(.4\\)\n\n\n\\((Bust, Nonviolent)\\)\n\\(.2\\)\n\n\n\\((Bust, Violent)\\)\n\\(.16\\)\n\n\n\\((Bust, Civil~War)\\)\n\\(.04\\)\n\n\n\n\n\n3.1.4 How are conditional, joint, and marginal probabilities related?\nJoint and conditional probabilities:\n\\[\\begin{align} \\label{standardization}\nP(Y=y,X=x)=P(X=x)P(Y=y|X=x)\\\\\n          =P(Y=y)P(X=x|Y=y)\n\\end{align}\\]\nJoint and marginal probabilities:\n\\[\\begin{align}\nP(X=x)=\\sum_y Pr(X=x,Y=y) \\\\\nP(Y=y)=\\sum_x Pr(X=x,Y=y)\n\\end{align}\\]\nFinding condtional probabilities from joints\nHere is the two-way probability table of above example.\n\n\n\n\nProbability Distribution of Economic and Conflict Scenarios\n\n\nCategory\nPeace\nNon_violent\nViolent\nCivil_War\nTotal\n\n\n\n\nBust\n0.40\n0.20\n0.160\n0.040\n0.8\n\n\nBoom\n0.16\n0.03\n0.006\n0.004\n0.2\n\n\nTotal\n0.56\n0.23\n0.166\n0.044\n1.0\n\n\n\n\n\n\n\n\n\nWhat is the probability of civil war outcome?\nWhat is the probability of civil war outcome knowing that the economy is busting?\n\nSolution: we can formally write these two questions as\n\\(P(S=Civil~War)=?\\) and \\(P(S=Civil~War|E=Bust)=?\\)\n\\[\\begin{align}\nP(S=Civil~War) = \\sum_{e \\in E} P(S=Civil~War,E=e) \\\\\n               = P(S=Civil~War,E=Bust)\\\\\n               +P(S=Civil~War,E=Boost)\\\\\n               = .04+.004=.044\n\\end{align}\\]\nTo find \\(P(S=Civil~War|E=Bust)\\), we can use \\(P(Y=y,X=x)=P(X=x)P(Y=y|X=x)\\):\n\n\n\n\nProbability Distribution of Economic and Conflict Scenarios\n\n\nCategory\nPeace\nNon_violent\nViolent\nCivil_War\nTotal\n\n\n\n\nBust\n0.40\n0.20\n0.160\nextcolor{red}{0.04}\n0.8\n\n\nBoom\n0.16\n0.03\n0.006\n0.004\n0.2\n\n\nTotal\n0.56\n0.23\n0.166\n0.044\n1.0\n\n\n\n\n\n\n\n\n\\[\\begin{align}\nP(S=Civil~War|E=Bust) = \\frac{P(S=Civil~War,E=Bust)}{p(E=Bust)}\\\\\n                      =  \\frac{.04}{.8}=.05=5\\%\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#naive-bayes-classification",
    "href": "NaiveBayes.html#naive-bayes-classification",
    "title": "3  Naive Bayes",
    "section": "3.3 Naive Bayes Classification",
    "text": "3.3 Naive Bayes Classification\nDespite its other name, “Idiot’s” Bayes Classification, this method often shows a very good classification performance!\nAssume that you have a categorical outcome \\(Y\\). As mentioned above, we are interested in finding \\(P(y|x)\\), that is what is the probability of \\(Y\\) conditional on observing \\(X=x\\).\nFor the sake of simplicity, assume that \\(Y=\\{0,1\\}\\). Then, the common practice among political scientists is estimating a logistic regression:\n\\[\nP(y=1|x) \\sim Bernoulli(p(x)), p(x)=\\frac{e^{x'\\beta}}{1+e^{x'\\beta}}\n\\]\nwhere \\(x=(x_1, x_2, \\dots, x_p)'\\) and \\(\\beta=(\\beta_1, \\beta_2, \\dots, \\beta_p)'\\).\nHowever, we can use Bayes Theorem to indirectly compute \\(P(y|x)\\). Naive Bayes classification makes a simplification assumption, which makes the name of this method naive: the elements of the features/explanatory variables of \\(X=(X_1, X_2, \\dots, X_p)\\) are conditionally independent given Y:\n\\[\np(Y=y|X=x)=\\frac{p(X=x|Y=y)p(Y=y)}{p(X=x)}\n\\] For continuous \\(x\\), we can re-write:\n\\[\np(Y=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^k f_l(x)}\n\\]\nGetting back to the discrete model; for a problem with \\(X=(X_1, X_2, \\dots, X_p)\\), we have:\n\\[\np(Y=y|X=x)=\\frac{p(\\{x_1, x_2, \\dots, x_p \\}|Y=y)p(Y=y)}{p(X=x)}\n\\]\nNaive Bayes assumes that \\(p(\\{x_1, x_2, \\dots, x_p \\}|Y=y)=\\prod_i p(x_i|y)\\). Thus,\n\\[\np(y|x) \\propto  p(y)\\prod_i p(x_i|y)\n\\]\nWhat is the interpretation of the above formula?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#an-example-spam-or-ham",
    "href": "NaiveBayes.html#an-example-spam-or-ham",
    "title": "3  Naive Bayes",
    "section": "3.4 An example: Spam or Ham?",
    "text": "3.4 An example: Spam or Ham?\nContent and sentiment analysis involve understanding the communicated messages in a text using computational methods. Indeed, the plan is to use a statistical model to train an algorithm to read, process, and analyze the text. Every word in a piece of text can add information about its intended message, tone, and sentiment.\nHere, we do a simple analysis of short message texts to categorize them to Spam and Ham. In this example, we use the Bag of Words method which does not consider the order of words, which contain information as well. However, surprisingly, this naive method is still a powerful too, as you see below.\nIn the Bag of Words, also known as BoW, method, these below texts are considered identical:\n\nSentence 1: There will be a presentation on Machine Learning in the school of management\nSentence 2: Learning will be a on Machine management in the school of There presentation.\n\nLet’s now do the coding part:\n\n## Loading the data of the web\nsmsData=read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/sms_spam.csv\", stringsAsFactors = FALSE, encoding=\"UTF-8\")\n\n\n## Let's take a look at the first few observations\n\nhead(smsData)\n\n  type\n1  ham\n2  ham\n3  ham\n4 spam\n5 spam\n6  ham\n                                                                                                                                                               text\n1                                                                                                                 Hope you are having a good week. Just checking in\n2                                                                                                                                           K..give back my thanks.\n3                                                                                                                       Am also doing in cbe only. But have to pay.\n4             complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\n5 okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm\n6                                                                                                                Aiya we discuss later lar... Pick u up at 4 is it?\n\nsummary(smsData$type)\n\n   Length     Class      Mode \n     5559 character character \n\nis.factor(smsData$type)\n\n[1] FALSE\n\n# We need to change the data type for \"type\" from text to factor\n\nsmsData$type=factor(smsData$type)\n\n# Let's look at the data now:\n\nsummary(smsData$type)\n\n ham spam \n4812  747 \n\nplot(smsData$type)\n\n\n\n\n\n\n\nis.factor(smsData$type)\n\n[1] TRUE\n\ndim(smsData)\n\n[1] 5559    2\n\nnames(smsData)\n\n[1] \"type\" \"text\"\n\nsmsData$type[1:5]\n\n[1] ham  ham  ham  spam spam\nLevels: ham spam\n\nsmsData$text[1:5]\n\n[1] \"Hope you are having a good week. Just checking in\"                                                                                                                \n[2] \"K..give back my thanks.\"                                                                                                                                          \n[3] \"Am also doing in cbe only. But have to pay.\"                                                                                                                      \n[4] \"complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\"            \n[5] \"okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm\"\n\n\nIn this question, the outcome variable is \\(y=type\\), and the feature is \\(x=text\\). We can check the frequency of words in the texts:\n\nlibrary(wordcloud)\n\n  wordcloud(smsData$text, min.freq = 1,\n          max.words=200, random.order=FALSE, rot.per=0, \n          colors=brewer.pal(8, \"Dark2\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "TreeModels.html",
    "href": "TreeModels.html",
    "title": "5  Tree Models: Random Forest",
    "section": "",
    "text": "5.1 Regression Trees\nIt is much easier to explain the tree models using an example. Let’s start with the Boston housing data that we used before in \\(kNN\\) section. Similarly, I want to predict median housing price in a neighborhood, \\({\\tt medv}\\), using the ratio of low state households, \\({\\tt lstat}\\).\nlibrary(MASS) \nattach(Boston)\nn = nrow(Boston)\n\nddf = data.frame(lstat,medv)\n\n# Sort the data here\noo=order(ddf$lstat)\n\nddf = ddf[oo,]\n\n\nlibrary(rpart)\n\nm_tree=rpart(medv~lstat,data=ddf)\n\n#plot the estimated tree\n\nsummary(m_tree)\n\nCall:\nrpart(formula = medv ~ lstat, data = ddf)\n  n= 506 \n\n          CP nsplit rel error    xerror       xstd\n1 0.44236500      0 1.0000000 1.0032163 0.08307058\n2 0.15283400      1 0.5576350 0.6176813 0.05295459\n3 0.06275014      2 0.4048010 0.4589917 0.04460663\n4 0.01374053      3 0.3420509 0.4027141 0.04204435\n5 0.01200979      4 0.3283103 0.3933751 0.04088310\n6 0.01159492      5 0.3163005 0.3885349 0.04088391\n7 0.01000000      6 0.3047056 0.3823150 0.04098829\n\nVariable importance\nlstat \n  100 \n\nNode number 1: 506 observations,    complexity param=0.442365\n  mean=22.53281, MSE=84.41956 \n  left son=2 (294 obs) right son=3 (212 obs)\n  Primary splits:\n      lstat &lt; 9.725  to the right, improve=0.442365, (0 missing)\n\nNode number 2: 294 observations,    complexity param=0.06275014\n  mean=17.34354, MSE=23.83089 \n  left son=4 (144 obs) right son=5 (150 obs)\n  Primary splits:\n      lstat &lt; 16.085 to the right, improve=0.3825785, (0 missing)\n\nNode number 3: 212 observations,    complexity param=0.152834\n  mean=29.72925, MSE=79.31047 \n  left son=6 (162 obs) right son=7 (50 obs)\n  Primary splits:\n      lstat &lt; 4.65   to the right, improve=0.3882819, (0 missing)\n\nNode number 4: 144 observations,    complexity param=0.01374053\n  mean=14.26181, MSE=18.74458 \n  left son=8 (75 obs) right son=9 (69 obs)\n  Primary splits:\n      lstat &lt; 19.9   to the right, improve=0.2174498, (0 missing)\n\nNode number 5: 150 observations\n  mean=20.302, MSE=10.84406 \n\nNode number 6: 162 observations,    complexity param=0.01159492\n  mean=26.6463, MSE=42.74335 \n  left son=12 (134 obs) right son=13 (28 obs)\n  Primary splits:\n      lstat &lt; 5.495  to the right, improve=0.07152825, (0 missing)\n\nNode number 7: 50 observations,    complexity param=0.01200979\n  mean=39.718, MSE=67.21788 \n  left son=14 (32 obs) right son=15 (18 obs)\n  Primary splits:\n      lstat &lt; 3.325  to the right, improve=0.1526421, (0 missing)\n\nNode number 8: 75 observations\n  mean=12.32533, MSE=16.18776 \n\nNode number 9: 69 observations\n  mean=16.36667, MSE=13.01729 \n\nNode number 12: 134 observations\n  mean=25.84701, MSE=40.14324 \n\nNode number 13: 28 observations\n  mean=30.47143, MSE=37.49776 \n\nNode number 14: 32 observations\n  mean=37.31562, MSE=65.92382 \n\nNode number 15: 18 observations\n  mean=43.98889, MSE=41.01765 \n\nsummary(ddf$medv)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00   17.02   21.20   22.53   25.00   50.00 \n\nplot(m_tree)\n\ntext(m_tree)\nBut, this plot does not often look good. Indeed, it usually is a mess! To plot a nice tree, I suggest using \\({\\tt rpart.plot}\\) package.\nlibrary(rpart.plot)\n\nrpart.plot(m_tree)\nAt each interior node, there is decision rule, \\(x&gt;c\\). If \\(x&gt;c\\), you should follow the branch on the left. This continues until you reach a bottom/terminal node, which also is known as the leaf of the tree.\nplot(ddf$lstat,ddf$medv, col='blue')\n\nlines(ddf$lstat,predict(m_tree,ddf),col='maroon',lwd=3)\n\n\nfor (i in m_tree$splits[,4] ){\nabline(v=i, col=\"orange\",lty=2)\n}\nThe set of bottom nodes gives us a partition of the predictor \\(x\\) space into disjoint regions. At right, the vertical lines display the partition. With just one x, this is just a set of intervals.\nWithin each region (interval) we compute the average of the \\(y\\) values for the subset of training data in the region. This gives us the step function which is our \\(\\hat{f}\\). The \\(\\bar{y}\\) values are also printed at the bottom nodes. To predict, we just use the above step function estimation of f(x).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#a-tree-with-two-explanatory-variables",
    "href": "TreeModels.html#a-tree-with-two-explanatory-variables",
    "title": "5  Tree Models: Random Forest",
    "section": "5.2 A Tree with Two Explanatory Variables",
    "text": "5.2 A Tree with Two Explanatory Variables\nNow, let’s estimate a tree model using two features: \\(x=(lstat, dis)\\) and \\(y=medx\\).\nFirst, let’s take a look at scatter plot of these variables:\n\nattach(Boston)\n\nddf = data.frame(medv,rm,nox,lstat, dis)\n\n# Sort the data here\noo=order(ddf$lstat)\n\nddf = ddf[oo,]\n\nlibrary(ggplot2)\n\nggplot(ddf, aes(lstat, dis, colour = medv)) + geom_point()\n\n\n\n\n\n\n\n\n\nm_tree2=rpart(medv~lstat+dis,data=ddf)\n\n\nrpart.plot(m_tree2, box.palette = \"Grays\")\n\n\n\n\n\n\n\nprint(m_tree2)\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 506 42716.3000 22.53281  \n   2) lstat&gt;=9.725 294  7006.2830 17.34354  \n     4) lstat&gt;=16.085 144  2699.2200 14.26181  \n       8) dis&lt; 2.0037 75  1114.6990 11.96933 *\n       9) dis&gt;=2.0037 69   761.9316 16.75362 *\n     5) lstat&lt; 16.085 150  1626.6090 20.30200 *\n   3) lstat&lt; 9.725 212 16813.8200 29.72925  \n     6) lstat&gt;=4.65 162  6924.4230 26.64630  \n      12) dis&gt;=2.4501 144  3493.3720 25.67986  \n        24) lstat&gt;=5.495 118  2296.9260 24.71695 *\n        25) lstat&lt; 5.495 26   590.4850 30.05000 *\n      13) dis&lt; 2.4501 18  2220.5910 34.37778 *\n     7) lstat&lt; 4.65 50  3360.8940 39.71800  \n      14) dis&gt;=3.20745 38  2087.6880 37.00789  \n        28) lstat&gt;=3.99 13   179.3908 32.06154 *\n        29) lstat&lt; 3.99 25  1424.8400 39.58000 *\n      15) dis&lt; 3.20745 12   110.3000 48.30000 *\n\nsummary(m_tree2)\n\nCall:\nrpart(formula = medv ~ lstat + dis, data = ddf)\n  n= 506 \n\n          CP nsplit rel error    xerror       xstd\n1 0.44236500      0 1.0000000 1.0054668 0.08318283\n2 0.15283400      1 0.5576350 0.6223409 0.05008953\n3 0.06275014      2 0.4048010 0.4597699 0.04089941\n4 0.02833720      3 0.3420509 0.4230489 0.04049512\n5 0.02722395      4 0.3137137 0.4258006 0.04066010\n6 0.01925703      5 0.2864897 0.4127407 0.03981422\n7 0.01418570      6 0.2672327 0.3518259 0.03450709\n8 0.01131786      7 0.2530470 0.3363448 0.03487452\n9 0.01000000      8 0.2417291 0.3292080 0.03334916\n\nVariable importance\nlstat   dis \n   72    28 \n\nNode number 1: 506 observations,    complexity param=0.442365\n  mean=22.53281, MSE=84.41956 \n  left son=2 (294 obs) right son=3 (212 obs)\n  Primary splits:\n      lstat &lt; 9.725   to the right, improve=0.4423650, (0 missing)\n      dis   &lt; 2.5977  to the left,  improve=0.1169235, (0 missing)\n  Surrogate splits:\n      dis &lt; 4.48025 to the left,  agree=0.737, adj=0.373, (0 split)\n\nNode number 2: 294 observations,    complexity param=0.06275014\n  mean=17.34354, MSE=23.83089 \n  left son=4 (144 obs) right son=5 (150 obs)\n  Primary splits:\n      lstat &lt; 16.085  to the right, improve=0.3825785, (0 missing)\n      dis   &lt; 2.0754  to the left,  improve=0.3369205, (0 missing)\n  Surrogate splits:\n      dis &lt; 2.2166  to the left,  agree=0.724, adj=0.438, (0 split)\n\nNode number 3: 212 observations,    complexity param=0.152834\n  mean=29.72925, MSE=79.31047 \n  left son=6 (162 obs) right son=7 (50 obs)\n  Primary splits:\n      lstat &lt; 4.65    to the right, improve=0.3882819, (0 missing)\n      dis   &lt; 2.16475 to the right, improve=0.1528193, (0 missing)\n  Surrogate splits:\n      dis &lt; 1.48495 to the right, agree=0.769, adj=0.02, (0 split)\n\nNode number 4: 144 observations,    complexity param=0.01925703\n  mean=14.26181, MSE=18.74458 \n  left son=8 (75 obs) right son=9 (69 obs)\n  Primary splits:\n      dis   &lt; 2.0037  to the left,  improve=0.3047506, (0 missing)\n      lstat &lt; 19.9    to the right, improve=0.2174498, (0 missing)\n  Surrogate splits:\n      lstat &lt; 19.73   to the right, agree=0.771, adj=0.522, (0 split)\n\nNode number 5: 150 observations\n  mean=20.302, MSE=10.84406 \n\nNode number 6: 162 observations,    complexity param=0.0283372\n  mean=26.6463, MSE=42.74335 \n  left son=12 (144 obs) right son=13 (18 obs)\n  Primary splits:\n      dis   &lt; 2.4501  to the right, improve=0.17481020, (0 missing)\n      lstat &lt; 5.495   to the right, improve=0.07152825, (0 missing)\n\nNode number 7: 50 observations,    complexity param=0.02722395\n  mean=39.718, MSE=67.21788 \n  left son=14 (38 obs) right son=15 (12 obs)\n  Primary splits:\n      dis   &lt; 3.20745 to the right, improve=0.3460110, (0 missing)\n      lstat &lt; 3.325   to the right, improve=0.1526421, (0 missing)\n  Surrogate splits:\n      lstat &lt; 1.95    to the right, agree=0.8, adj=0.167, (0 split)\n\nNode number 8: 75 observations\n  mean=11.96933, MSE=14.86266 \n\nNode number 9: 69 observations\n  mean=16.75362, MSE=11.04249 \n\nNode number 12: 144 observations,    complexity param=0.0141857\n  mean=25.67986, MSE=24.25952 \n  left son=24 (118 obs) right son=25 (26 obs)\n  Primary splits:\n      lstat &lt; 5.495   to the right, improve=0.17346010, (0 missing)\n      dis   &lt; 4.39775 to the right, improve=0.07588735, (0 missing)\n\nNode number 13: 18 observations\n  mean=34.37778, MSE=123.3662 \n\nNode number 14: 38 observations,    complexity param=0.01131786\n  mean=37.00789, MSE=54.93915 \n  left son=28 (13 obs) right son=29 (25 obs)\n  Primary splits:\n      lstat &lt; 3.99    to the right, improve=0.2315753, (0 missing)\n      dis   &lt; 6.40075 to the right, improve=0.1173026, (0 missing)\n\nNode number 15: 12 observations\n  mean=48.3, MSE=9.191667 \n\nNode number 24: 118 observations\n  mean=24.71695, MSE=19.46548 \n\nNode number 25: 26 observations\n  mean=30.05, MSE=22.71096 \n\nNode number 28: 13 observations\n  mean=32.06154, MSE=13.79929 \n\nNode number 29: 25 observations\n  mean=39.58, MSE=56.9936 \n\n\n\nlibrary(tree)\n\nm_tree2=tree(medv~lstat+dis,data=ddf)\n\nsummary(m_tree2)\n\n\nRegression tree:\ntree(formula = medv ~ lstat + dis, data = ddf)\nNumber of terminal nodes:  9 \nResidual mean deviance:  20.78 = 10330 / 497 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-14.780  -2.713  -0.302   0.000   2.298  15.620 \n\npartition.tree(m_tree2)\n\n\n\n\n\n\n\n\n\nattach(Boston)\nmid&lt;-mean(medv)\nggplot(ddf, aes(lstat, rm, colour = medv)) + geom_point()+scale_color_gradient2(midpoint=mid, low=\"blue\", mid=\"cyan\",\n                     high=\"maroon\", space =\"Lab\" )\n\n\n\n\n\n\n\nm_tree31=rpart(medv~lstat+rm+dis,data=ddf)\n\nsummary(m_tree31)\n\nCall:\nrpart(formula = medv ~ lstat + rm + dis, data = ddf)\n  n= 506 \n\n          CP nsplit rel error    xerror       xstd\n1 0.45274420      0 1.0000000 1.0048956 0.08303782\n2 0.17117244      1 0.5472558 0.6464139 0.05797184\n3 0.07165784      2 0.3760834 0.4177925 0.04498306\n4 0.03616428      3 0.3044255 0.3304384 0.04098136\n5 0.03336923      4 0.2682612 0.3145039 0.04204864\n6 0.02311607      5 0.2348920 0.3037815 0.04110406\n7 0.01585116      6 0.2117759 0.3062113 0.04247762\n8 0.01000000      7 0.1959248 0.2846706 0.04098910\n\nVariable importance\n   rm lstat   dis \n   54    33    13 \n\nNode number 1: 506 observations,    complexity param=0.4527442\n  mean=22.53281, MSE=84.41956 \n  left son=2 (430 obs) right son=3 (76 obs)\n  Primary splits:\n      rm    &lt; 6.941   to the left,  improve=0.4527442, (0 missing)\n      lstat &lt; 9.725   to the right, improve=0.4423650, (0 missing)\n      dis   &lt; 2.5977  to the left,  improve=0.1169235, (0 missing)\n  Surrogate splits:\n      lstat &lt; 4.83    to the right, agree=0.891, adj=0.276, (0 split)\n\nNode number 2: 430 observations,    complexity param=0.1711724\n  mean=19.93372, MSE=40.27284 \n  left son=4 (175 obs) right son=5 (255 obs)\n  Primary splits:\n      lstat &lt; 14.4    to the right, improve=0.4222277, (0 missing)\n      dis   &lt; 2.58835 to the left,  improve=0.2036255, (0 missing)\n      rm    &lt; 6.5455  to the left,  improve=0.1442877, (0 missing)\n  Surrogate splits:\n      dis &lt; 2.23935 to the left,  agree=0.781, adj=0.463, (0 split)\n      rm  &lt; 5.858   to the left,  agree=0.688, adj=0.234, (0 split)\n\nNode number 3: 76 observations,    complexity param=0.07165784\n  mean=37.23816, MSE=79.7292 \n  left son=6 (46 obs) right son=7 (30 obs)\n  Primary splits:\n      rm    &lt; 7.437   to the left,  improve=0.50515690, (0 missing)\n      lstat &lt; 4.68    to the right, improve=0.33189140, (0 missing)\n      dis   &lt; 6.75885 to the right, improve=0.03479472, (0 missing)\n  Surrogate splits:\n      lstat &lt; 3.99    to the right, agree=0.776, adj=0.433, (0 split)\n\nNode number 4: 175 observations,    complexity param=0.02311607\n  mean=14.956, MSE=19.27572 \n  left son=8 (86 obs) right son=9 (89 obs)\n  Primary splits:\n      dis   &lt; 2.0037  to the left,  improve=0.2927244, (0 missing)\n      lstat &lt; 19.83   to the right, improve=0.2696497, (0 missing)\n      rm    &lt; 5.567   to the left,  improve=0.0750970, (0 missing)\n  Surrogate splits:\n      lstat &lt; 19.73   to the right, agree=0.749, adj=0.488, (0 split)\n      rm    &lt; 5.5505  to the left,  agree=0.634, adj=0.256, (0 split)\n\nNode number 5: 255 observations,    complexity param=0.03616428\n  mean=23.3498, MSE=26.0087 \n  left son=10 (248 obs) right son=11 (7 obs)\n  Primary splits:\n      dis   &lt; 1.5511  to the right, improve=0.2329242, (0 missing)\n      lstat &lt; 4.91    to the right, improve=0.2208409, (0 missing)\n      rm    &lt; 6.543   to the left,  improve=0.2172099, (0 missing)\n\nNode number 6: 46 observations,    complexity param=0.01585116\n  mean=32.11304, MSE=41.29592 \n  left son=12 (7 obs) right son=13 (39 obs)\n  Primary splits:\n      lstat &lt; 9.65    to the right, improve=0.35644260, (0 missing)\n      dis   &lt; 3.45845 to the left,  improve=0.08199333, (0 missing)\n      rm    &lt; 7.3     to the right, improve=0.05938584, (0 missing)\n  Surrogate splits:\n      dis &lt; 1.6469  to the left,  agree=0.87, adj=0.143, (0 split)\n\nNode number 7: 30 observations\n  mean=45.09667, MSE=36.62832 \n\nNode number 8: 86 observations\n  mean=12.53953, MSE=16.13867 \n\nNode number 9: 89 observations\n  mean=17.29101, MSE=11.21228 \n\nNode number 10: 248 observations,    complexity param=0.03336923\n  mean=22.93629, MSE=14.75159 \n  left son=20 (193 obs) right son=21 (55 obs)\n  Primary splits:\n      rm    &lt; 6.543   to the left,  improve=0.38962730, (0 missing)\n      lstat &lt; 7.685   to the right, improve=0.33560120, (0 missing)\n      dis   &lt; 2.8405  to the left,  improve=0.03769213, (0 missing)\n  Surrogate splits:\n      lstat &lt; 5.055   to the right, agree=0.839, adj=0.273, (0 split)\n      dis   &lt; 10.648  to the left,  agree=0.782, adj=0.018, (0 split)\n\nNode number 11: 7 observations\n  mean=38, MSE=204.1457 \n\nNode number 12: 7 observations\n  mean=23.05714, MSE=61.85673 \n\nNode number 13: 39 observations\n  mean=33.73846, MSE=20.24391 \n\nNode number 20: 193 observations\n  mean=21.65648, MSE=8.23738 \n\nNode number 21: 55 observations\n  mean=27.42727, MSE=11.69398 \n\nrpart.plot(m_tree31, box.palette = \"Grays\")\n\n\n\n\n\n\n\n\n\nm_tree32=rpart(medv~lstat+rm+dis,data=ddf)\n\n\nrpart.plot(m_tree32, box.palette = \"Grays\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#what-is-the-loss-problem-and-optimization-question-in-tree-algorithm",
    "href": "TreeModels.html#what-is-the-loss-problem-and-optimization-question-in-tree-algorithm",
    "title": "5  Tree Models: Random Forest",
    "section": "5.3 What is the loss problem and optimization question in tree algorithm?",
    "text": "5.3 What is the loss problem and optimization question in tree algorithm?\nAs shown above, we want to split the feature space, i.e. \\(X=\\{x_1, x_2, \\dots, x_k\\}\\), to smaller rectangular, or boxes. If we find a splitting pattern which has the lowest prediction error for the training data, then we can predict the associated outcome with the prediction feature set \\(X_p\\) by finding to which box this set belongs. Roughly, we can think about about tree algorithm like a \\(kNN\\) algorithm in which \\(k\\) is dynamic and can change according to the changes in the data.\n\\[\\begin{align}\n\nmin_{R_1,R_2,\\dots,R_J} \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i-\\hat{y}_{R_j})^2\n\n\\end{align}\\]\nThis loss has a straightforward intuition: we want to find \\(R_j\\)s to minimize the sum of the squared deviation of observations from the mean response within each box. In theory, we can define infinite number of boxes, and we need to evaluate the squared errors of each set of boxes to see which one is minimized. This is computationally very intensive and almost impossible.\nTo solve this problem, a recursive partitioning top-down approach is used to find the optimal boxes:\n\nScan each feature/predictor in the data to find the one for which splitting gives the best performance/prediction power, i.e. lowest error.\nNow, for each split repeat the previous step, and continue this until splitting is too costly (we will discuss later what “costly” means, but just as a tip: think about bias-variance tradeoff), or reach a pre-defined tree size.\n\nThis is a greedy approach since the splitting process only focus on each step and pick the split that is associated with the best fit at the current step, rather than looking ahead and picking the split which lead to a better prediction tree in the following steps.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#how-to-fit-the-data-to-a-tree-model",
    "href": "TreeModels.html#how-to-fit-the-data-to-a-tree-model",
    "title": "5  Tree Models: Random Forest",
    "section": "5.4 How to fit the data to a tree model?",
    "text": "5.4 How to fit the data to a tree model?\nTo execute the above algorithm, statistical algorithms re-write the above problem as follow:\n\\[\\begin{align}\n\nC(T,y)=L(T,y)+\\alpha |T|\n\n\\end{align}\\]\nwhere L(T,y) is the loss of fitting outcome \\(y\\) with tree \\(T\\). Our goal always is minimizing the loss, that is small \\(L\\) is preferred.\nHowever, we do not want to make the model/tree too complex. A complex model might be good in capturing all variances in the training data, but would lead to a biased prediction of in the test data. Thus, we add the number of nodes in tree T, i.e. \\(|T|\\), with a penalty parameter \\(\\alpha\\) to the cost function \\(C(T,y)\\). For the continuous \\(y\\), we can use RMSE, and for categorical \\(y\\), we can use any of the classification measures that are discussed in the previous section.\nWhat is the best \\(\\alpha\\)? if we pick a big \\(\\alpha\\), then the penalty for having a big and complex tree is large. Thus, the optimization problem return a small tree which can lead to a large \\(L\\) on the training data. On the other hand, for a small \\(\\alpha\\), we allow the model pick a big tree. Here, \\(\\alpha\\) is analogous to \\(k\\) in \\(kNN\\). How do we pick \\(\\alpha\\)? The answer similar to most of the similar cases in this course is cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#pruning-trees",
    "href": "TreeModels.html#pruning-trees",
    "title": "5  Tree Models: Random Forest",
    "section": "5.5 Pruning trees",
    "text": "5.5 Pruning trees\nThe idea here is first allow a tree grows as much as possible without being worried about its complexity size. Then, considering the complexity parameter, CP value, we prune/cut the tree with the optimal CP value.\n\nattach(Boston)\n\ndf=Boston[,c(8,13,14)] # pick dis, lstat, medv\n\nprint(names(df))\n\n[1] \"dis\"   \"lstat\" \"medv\" \n\n# First grow a big tree\n\ntree_ml=tree(medv~., df)\n\nprint(tree_ml)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 506 42720.0 22.53  \n   2) lstat &lt; 9.725 212 16810.0 29.73  \n     4) lstat &lt; 4.65 50  3361.0 39.72  \n       8) dis &lt; 3.20745 12   110.3 48.30 *\n       9) dis &gt; 3.20745 38  2088.0 37.01  \n        18) lstat &lt; 3.99 25  1425.0 39.58 *\n        19) lstat &gt; 3.99 13   179.4 32.06 *\n     5) lstat &gt; 4.65 162  6924.0 26.65  \n      10) dis &lt; 2.4501 18  2221.0 34.38 *\n      11) dis &gt; 2.4501 144  3493.0 25.68  \n        22) lstat &lt; 5.495 26   590.5 30.05 *\n        23) lstat &gt; 5.495 118  2297.0 24.72 *\n   3) lstat &gt; 9.725 294  7006.0 17.34  \n     6) lstat &lt; 16.085 150  1627.0 20.30 *\n     7) lstat &gt; 16.085 144  2699.0 14.26  \n      14) dis &lt; 2.0037 75  1115.0 11.97 *\n      15) dis &gt; 2.0037 69   761.9 16.75 *\n\nplot(tree_ml)\ntext(tree_ml)\n\n\n\n\n\n\n\ncat(\"Size of the big tree: \\n\")\n\nSize of the big tree: \n\nprint(length(unique(tree_ml$where)))\n\n[1] 9\n\nBoston_fit=predict(tree_ml,df)\n\n# Now, let's prune it down to a tree with 6 leaves/nodes\n\ntree_ml_7=prune.tree(tree_ml,best=7)\n\ncat(\"Size of the big tree: \\n\")\n\nSize of the big tree: \n\nprint(length(unique(tree_ml_7$where)))\n\n[1] 7\n\npar(mfrow=c(1,2))\n\nplot(tree_ml_7, type='u')\ntext(tree_ml_7, col='maroon', label=c('yval'), cex=.9)\n\n\nBoston_fit7=predict(tree_ml_7,df)\n\n##\n\nResultsMat=cbind(medv,Boston_fit,Boston_fit7)\n\ncolnames(ResultsMat)=c(\"medv\",\"tree\",\"tree7\")\n\npairs(ResultsMat, col='maroon')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(cor(ResultsMat))\n\n           medv      tree     tree7\nmedv  1.0000000 0.8707875 0.8560183\ntree  0.8707875 1.0000000 0.9830393\ntree7 0.8560183 0.9830393 1.0000000\n\n#Let's use the trained algorithm to predict the value for a specific point\n\nPred_point=data.frame(lstat=15,dis=2)\n\nyhat=predict(tree_ml,Pred_point)\nyhat_7=predict(tree_ml_7,Pred_point)\n\n\ncat('prediction is: \\n')\n\nprediction is: \n\nprint(yhat)\n\n     1 \n20.302 \n\nprint(yhat_7)\n\n     1 \n20.302",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#a-cross-validated-problem",
    "href": "TreeModels.html#a-cross-validated-problem",
    "title": "5  Tree Models: Random Forest",
    "section": "5.6 A cross-validated problem",
    "text": "5.6 A cross-validated problem\n\nattach(Boston)\nset.seed(7)\n\ndf=Boston[,c(8,13,14)]\n\nprint(names(df))\n\n[1] \"dis\"   \"lstat\" \"medv\" \n\n# Let's fit a single tree and plot the importance of the variables\n\ntree=rpart(medv~., method=\"anova\",data=df,\n           control=rpart.control(minsplit = 5,cp=.0005))\n\n\nNtree=length(unique(tree$where))\n\ncat(\"Size of big tree is:\",Ntree,\"\\n\")\n\nSize of big tree is: 90 \n\n# Let's check the CV results\n\nplotcp(tree)\n\n\n\n\n\n\n\nbest_a=which.min(tree$cptable[,\"xerror\"])\n\nbestCP=tree$cptable[best_a,\"CP\"]\n\nbestSize=tree$cptable[best_a,\"nsplit\"]+1\n\nbest_tree=prune(tree,cp=bestCP)\n\nplot(best_tree,uniform=TRUE)\ntext(best_tree,digits=2, use.n = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#bagging-and-random-forest",
    "href": "TreeModels.html#bagging-and-random-forest",
    "title": "5  Tree Models: Random Forest",
    "section": "5.7 Bagging and Random Forest",
    "text": "5.7 Bagging and Random Forest\nThe idea behind Bagging is bootstrapping the data adequately enough to make sure that trees with good explanatory power are captured. This approach is computationally intensive and time-consuming.\nRandom Forests starts from Bagging and adds another kind of randomization. In this method, instead of going over all features when we do the greedy approach, this method randomly samples a subset of \\(m\\) variables to search over each time we make a split.\nIn this method, more types of trees will be evaluated. Since this method is Bootstraped, the important variables will be identified in average more than others; and thus can be used for prediction.\nHow to choose parameters for a Random Forest model? \\(B\\) is the number of Bootstrapped samples, and \\(m\\) is the number of variables to sample. A common choice for \\(m\\) is \\(\\sqrt p\\), where \\(p\\) is the number of features in the model. When we set \\(m=p\\), then Random Forest is Bagging.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "TreeModels.html#california-housing-data",
    "href": "TreeModels.html#california-housing-data",
    "title": "5  Tree Models: Random Forest",
    "section": "5.8 California Housing Data",
    "text": "5.8 California Housing Data\n\nlibrary(randomForest)\n\n\nrawData=read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/calhouse.csv\")\n\n# First divide the sample to train, validation, and test samples\n\n\nset.seed(7)\n\n\nn=nrow(rawData)\n\nn1=floor(n/2)\nn2=floor(n/4)\nn3=n-n1-n2\n\n#Shuffle the data\nii=sample(1:n,n)\n\nCAtrain=rawData[ii[1:n1],]\nCAval=rawData[ii[n1+1:n2],]\nCAtest=rawData[ii[n1+n2+1:n3],]\n\n## Fitting using the RF on train and evaluate on the tes, and predict on val\n\nRF=randomForest(logMedVal~.,data=CAtrain,mtry=3,ntree=500)\n\nRFpred=predict(RF,newdata=CAval)\n\nRMSE=sqrt(mean(CAval$logMedVal-RFpred)^2)\n\ncat('RMSE on the train data for Random Forest is',RMSE,\"\\n\")\n\nRMSE on the train data for Random Forest is 0.00363087 \n\ngetTree(RF, 1, labelVar=TRUE)[1:30,]\n\n   left daughter right daughter        split var split point status prediction\n1              2              3     medianIncome    3.574250     -3  12.086736\n2              4              5     AveOccupancy    2.294164     -3  11.775037\n3              6              7     AveOccupancy    2.783601     -3  12.417998\n4              8              9         latitude   37.975000     -3  12.053484\n5             10             11         AveRooms    4.350556     -3  11.700731\n6             12             13         AveRooms    6.438173     -3  12.556428\n7             14             15     medianIncome    5.745500     -3  12.294984\n8             16             17        longitude -117.135000     -3  12.163971\n9             18             19       households   14.500000     -3  11.674856\n10            20             21       households  446.500000     -3  11.850795\n11            22             23        longitude -118.895000     -3  11.619754\n12            24             25        longitude -117.175000     -3  12.500131\n13            26             27        AveBedrms    1.809925     -3  12.724364\n14            28             29     medianIncome    4.574500     -3  12.129722\n15            30             31 housingMedianAge   27.500000     -3  12.696060\n16            32             33 housingMedianAge   24.500000     -3  12.250378\n17            34             35        longitude -116.180000     -3  11.493662\n18             0              0             &lt;NA&gt;    0.000000     -1   9.615739\n19            36             37        longitude -122.425000     -3  11.683028\n20            38             39     medianIncome    1.854500     -3  11.730831\n21            40             41        longitude -117.795000     -3  11.971906\n22            42             43 housingMedianAge   48.500000     -3  11.531629\n23            44             45         latitude   34.425000     -3  11.759766\n24            46             47         latitude   37.995000     -3  12.516257\n25            48             49     medianIncome    4.595600     -3  12.057731\n26            50             51 housingMedianAge    9.500000     -3  12.797211\n27            52             53        longitude -121.525000     -3  12.182172\n28            54             55     AveOccupancy    2.784340     -3  12.027770\n29            56             57        longitude -122.225000     -3  12.252785\n30            58             59         AveRooms    7.908003     -3  12.648332\n\npairs(cbind(CAval$logMedVal,RFpred))\n\n\n\n\n\n\n\nprint(cor(cbind(CAval$logMedVal,RFpred)))\n\n                    RFpred\n       1.0000000 0.9056696\nRFpred 0.9056696 1.0000000\n\nvarImpPlot(RF)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree Models: Random Forest</span>"
    ]
  },
  {
    "objectID": "knn.html#a-simple-comparison-of-ols-and-k-nn-boston-housing-example",
    "href": "knn.html#a-simple-comparison-of-ols-and-k-nn-boston-housing-example",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.2 A simple Comparison of OLS and k-NN: Boston Housing Example",
    "text": "2.2 A simple Comparison of OLS and k-NN: Boston Housing Example\nThe Boston Housing Dataset contains information on housing in the Boston, Massachusetts area in the United States. Suppose we want to predict the median value of a house based on the percentage of the population experiencing socioeconomic disadvantage in a given neighborhood. In the original dataset, this variable is labeled as “lower status,” reflecting the historical language used when the dataset was created in the 1970s. However, modern academic and professional standards call for more thoughtful and respectful terminology when discussing socioeconomic conditions. Therefore, I have relabeled this variable as “socioeconomic disadvantage population” to provide a more accurate and considerate description. The below scatter plot shows how these two variables are associated:\n\nlibrary(MASS) ## a library of example datasets\nattach(Boston)\n\nplot(lstat,medv,\n     col='navy',cex=.75,\n     xlab='Socioeconomic Disadvantage Population(%)',ylab = 'Median Value($)')\n\n\n\n\n\n\n\n\n\n2.2.1 Linear Model for Housing Values\nTo analyze how housing values are associated with the percentage of the population experiencing socioeconomic disadvantage in a neighborhood, we need to define a function that models this relationship. A linear function is often a reasonable starting point. In this case, we assume the following linear model:\n\\[\nY_i = \\alpha + \\beta X_i + \\varepsilon_i\n\\] Here, \\(Y_i\\) represents the housing value, \\(X_i\\) is the percentage of socioeconomic disadvantage, \\(\\alpha\\) is the intercept, \\(\\beta\\) is the slope coefficient, and \\(\\varepsilon_i\\) is the error term.\nAfter estimating the parameters of this model—denoted as \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)—the predicted housing values are given by the Ordinary Least Squares (OLS) estimation formula:\n\\[\n\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\n\\]\nSince we have assumed a linear relationship between \\(X\\) and \\(Y\\), the fitted prediction line is also linear. This is reflected in the plot below, where the blue line represents the OLS-fitted regression line.\n\nplot(lstat,medv,\n      col='grey',cex=.75,\n     xlab='Socioeconomic Disadvantage Population (%)',ylab = 'Median Value($)', main=TeX('$Y=\\\\alpha + \\\\beta X$'))\n\nlmHousing=lm(medv~lstat)\nabline(lmHousing$coef, col=\"navy\", lwd=2.5)\nyhat_lm25=predict(lmHousing,data.frame(lstat=c(20)))\npoints(20,yhat_lm25,cex=1, col=\"maroon\", pch=15)\ntext(20,yhat_lm25,paste(round(yhat_lm25,digits = 2)),pos=3, cex=1,col=\"maroon\")\n\n\n\n\n\n\n\n\nWhen using OLS for prediction, we make several key assumptions about the form of the relationship between \\(X\\) and \\(Y\\), denoted as \\(Y = f(X)\\). These assumptions include linearity, homoscedasticity, independence of errors, and normality of residuals. The same applies to other parametric models, such as Logit and Probit, which assume specific functional forms for modeling binary outcomes.\nHowever, non-parametric approaches impose fewer assumptions on the functional form of \\(f(X)\\). This flexibility allows them to model complex, non-linear relationships without requiring a predetermined structure. One of the most commonly used non-parametric algorithms is the \\(K\\)-Nearest Neighbors (\\(K\\)-NN) algorithm.\nBefore diving into the details of this method, let’s first explore how \\(K\\)-NN fits the function \\(f(\\cdot)\\) to the data, relying on the proximity of data points rather than predefined functional forms.\n\n#library(MASS) ## a library of example datasets\n#attach(Boston)\n\nlibrary(kknn) ## knn library\n\n\n# We first need to divide the sample to train (in-sample) and test (out-sample) subsampes. \n\nn = nrow(Boston) # Sample size\n\n#fit knn with k\ntrain = data.frame(lstat,medv) #data frame with variables of interest\n#test is data frame with x you want f(x) at, sort lstat to make plots nice.\ntest = data.frame(lstat = sort(lstat))\nkf15 = kknn(medv~lstat,train,test,k=15,kernel = \"rectangular\")\n\nplot(lstat,medv,xlab='Socioeconomic Disadvantage Population (%)',ylab = 'Median Value($)', col=\"gray45\")\n\nlines(test$lstat,kf15$fitted.values,col=\"blue\",lwd=2)\n\n\ndfp = data.frame(lstat=20)\n\nkf15_20 = kknn(medv~lstat,train,dfp,k=15,kernel = \"rectangular\")\ncat(\"kNN15: the median value of a house in a neighborhood with 20% Socioeconomic Disadvantage Population residents is\",round(kf15_20$fitted,digits=2),\"\\n\")\n\nkNN15: the median value of a house in a neighborhood with 20% Socioeconomic Disadvantage Population residents is 14.37 \n\npoints(20,kf15_20$fitted,cex=1, col=\"maroon\", pch=15)\ntext(20,kf15_20$fitted,paste(round(kf15_20$fitted,digits = 2)),pos=3, cex=1,col=\"maroon\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "knn.html#how-does-k-nn-estimate-fcdot",
    "href": "knn.html#how-does-k-nn-estimate-fcdot",
    "title": "2  K-Nearest Neighbors (KNN)",
    "section": "2.3 How Does \\(K\\)-NN Estimate \\(f(\\cdot)\\)?",
    "text": "2.3 How Does \\(K\\)-NN Estimate \\(f(\\cdot)\\)?\nTo estimate \\(f(\\cdot)\\) using \\(K\\)-Nearest Neighbors (\\(K\\)-NN), we first need to define a training dataset based on our main sample. The training set for a \\(K\\)-NN algorithm consists of \\(N\\) pairs of features (\\(x\\)) and outcomes (\\(y\\)) as follows:\n\\[\n\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\n\\]\nWe begin with a single-feature model for simplicity. Later in this handout, we will extend the \\(K\\)-NN algorithm to handle cases with multiple features, where \\(j &gt; 1\\).\nThe \\(K\\)-NN algorithm estimates \\(f(x_i)\\) by calculating the average of the outcome values for the \\(K\\) nearest neighbors to \\(x_i\\). This approach relies on the idea that observations with similar feature values tend to have similar outcomes, allowing \\(K\\)-NN to make predictions without assuming a specific functional form for the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>K-Nearest Neighbors (KNN)</span>"
    ]
  },
  {
    "objectID": "HW1.html",
    "href": "HW1.html",
    "title": "5  Homework 1",
    "section": "",
    "text": "5.1 Question 1: Setting Up Your GitHub Repository\nGitHub is a popular cloud-based platform used by software engineers and data scientists use to document projects and collaborate effectively. As a free repository, GitHub can also help you document your work for this course. For example, rather than submitting datasets directly to me, you’ll learn how to read data from cloud repositories like GitHub and Dropbox.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#question-1-setting-up-your-github-repository",
    "href": "HW1.html#question-1-setting-up-your-github-repository",
    "title": "5  Homework 1",
    "section": "",
    "text": "5.1.1 Instructions:\na. Watch this video and create a GitHub account.\n**b.* Watch this video.\n- Note: You do NOT need to set up the desktop version, so feel free to skip that part.\n\nNow, create a new repository and label it as: LU_BAP_CIR\n\nc. Download the Acemoglu_Robinson_2001 dataset.csv via this OneDrive link:\nDownload Dataset Here\n\nCreate a new folder within your LU_BAP_CIR repository and name it: datasets\nUpload the downloaded *Acemoglu_Robinson_2001 dataset** into the Datasets folder.\n\n\n\n5.1.2 Submission:\nShare the link to your GitHub profile as your answer. For example, mine looks like this:\nhttps://github.com/babakrezaee\nI expect to see the following structure in your GitHub repository: - A repository named LU_BAP_CIR - A folder named datasets within the repository - The Acemoglu_Robinson_2001 dataset.csv uploaded inside the Datasets folder",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#question-2-clean-up-the-data-and-visualize",
    "href": "HW1.html#question-2-clean-up-the-data-and-visualize",
    "title": "6  Homework 1",
    "section": "6.2 Question 2: Clean up the data and visualize",
    "text": "6.2 Question 2: Clean up the data and visualize\nIn a paper in Research & Politics, my co-author and I explore the effects of providing information on individuals support for nonviolent resistance in an experimental survey. The draft of the paper is available here. We will use a modified version of the collected data in this study. The collected data is available from my GitHub page: here\nBefore assigning the participants in the experiment to control and treatment groups, we asked some questions to measure some of important socio-economic factors and pre-experiment attitude toward nonviolent and violent methods of resistance. We used a set of questions to measure participants attitude toward (non)violent method of resistance. For this assignment, you can only focus on violent_method. Let’s load the data and do some descrptive analysis.\n\ndata_cl=read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/LeidenUniv_MAQM2020/Datasets/R%26P_RezaeeAsadzadeh_MethodsCourse.csv\")\n\nlibrary(ggplot2)\n\nnrow(data_cl)\n\n[1] 436\n\nsummary(data_cl$Violent_method)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.900   4.081   5.850  10.000       1 \n\n# Kernel Density Plot for 'Violent_method'\nggplot(data_cl, aes(x = Violent_method)) +\n  geom_density(kernel = \"gaussian\", fill = \"darkred\", alpha = 0.6, adjust = 1) +\n  labs(\n    title = \"Kernel Density Estimation (KDE) of Violent Method\",\n    x = \"Violent Method\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUse class lectures to train a knn algorithm that predicts attitude toward violent methods of resiatnce, i.e. Violent_method. When preparing you answers, make sure that you algorithm is cross-validated. Discuss your decisions and discuss your findings. The number of features should be at least three.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#footnotes",
    "href": "HW1.html#footnotes",
    "title": "6  Homework 1",
    "section": "",
    "text": "Daron Acemoglu, Simon Johnson, and James A. Robinson. The Colonial Origins of Comparative Development: An Empirical Investigation. The American Economic Review, 91(5):1369–1401, 2001. http://www.jstor.org/stable/26779306↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#question-2-attitude-toward-violent-method-of-resiatnce",
    "href": "HW1.html#question-2-attitude-toward-violent-method-of-resiatnce",
    "title": "5  Homework 1",
    "section": "5.2 Question 2: Attitude toward violent method of resiatnce",
    "text": "5.2 Question 2: Attitude toward violent method of resiatnce\nIn a paper published in Research & Politics, my co-author and I explore the effects of providing information on individuals’ support for nonviolent resistance through an experimental survey. You can access the draft of the paper here.\nFor this assignment, we will use a modified version of the dataset collected for this study. The dataset is publicly available on my GitHub page and can be accessed here.\n\n\n5.2.1 Dataset Description\nBefore assigning participants in the experiment to control and treatment groups, we collected data on several socio-economic factors and participants’ pre-experiment attitudes toward both nonviolent and violent methods of resistance.\nFor this assignment, we will focus on analyzing the variable Violent_method, which measures participants’ attitudes toward violent methods of resistance. Let’s begin by loading the dataset and performing some descriptive analysis.\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Load the dataset\ndata_cl &lt;- read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/LeidenUniv_MAQM2020/Datasets/R%26P_RezaeeAsadzadeh_MethodsCourse.csv\")\n\n# Basic Descriptive Statistics\nnrow(data_cl)                        # Number of observations\n\n[1] 436\n\nsummary(data_cl$Violent_method)      # Summary statistics for Violent_method\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.900   4.081   5.850  10.000       1 \n\n# Kernel Density Plot for 'Violent_method'\nggplot(data_cl, aes(x = Violent_method)) +\n  geom_density(kernel = \"gaussian\", fill = \"darkred\", alpha = 0.6, adjust = 1) +\n  labs(\n    title = \"Kernel Density Estimation (KDE) of Violent Method\",\n    x = \"Violent Method\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.2.2 Assignment: Predicting Attitudes Using k-NN\n\nModel Development:\nUse at least three features from the dataset to build your predictive model. Ensure your k-NN algorithm is cross-validated to assess its performance effectively.\nDocumentation:\nClearly explain the decisions you made throughout the process, including your choice of features, parameter settings (e.g., the value of k), and data preprocessing steps.\nAnalysis: Discuss your findings in detail. Reflect on the performance of your model, the influence of different features on predictions, and any challenges you encountered during the analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#dataset-description",
    "href": "HW1.html#dataset-description",
    "title": "6  Homework 1",
    "section": "7.1 Dataset Description",
    "text": "7.1 Dataset Description\nBefore assigning participants in the experiment to control and treatment groups, we collected data on several socio-economic factors and participants’ pre-experiment attitudes toward both nonviolent and violent methods of resistance.\nFor this assignment, we will focus on analyzing the variable Violent_method, which measures participants’ attitudes toward violent methods of resistance. Let’s begin by loading the dataset and performing some descriptive analysis.\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Load the dataset\ndata_cl &lt;- read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/LeidenUniv_MAQM2020/Datasets/R%26P_RezaeeAsadzadeh_MethodsCourse.csv\")\n\n# Basic Descriptive Statistics\nnrow(data_cl)                        # Number of observations\n\n[1] 436\n\nsummary(data_cl$Violent_method)      # Summary statistics for Violent_method\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.900   4.081   5.850  10.000       1 \n\n# Kernel Density Plot for 'Violent_method'\nggplot(data_cl, aes(x = Violent_method)) +\n  geom_density(kernel = \"gaussian\", fill = \"darkred\", alpha = 0.6, adjust = 1) +\n  labs(\n    title = \"Kernel Density Estimation (KDE) of Violent Method\",\n    x = \"Violent Method\",\n    y = \"Density\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#assignment-predicting-attitudes-using-k-nn",
    "href": "HW1.html#assignment-predicting-attitudes-using-k-nn",
    "title": "6  Homework 1",
    "section": "7.2 Assignment: Predicting Attitudes Using k-NN",
    "text": "7.2 Assignment: Predicting Attitudes Using k-NN\n\nModel Development:\nUse at least three features from the dataset to build your predictive model. Ensure your k-NN algorithm is cross-validated to assess its performance effectively.\nDocumentation:\nClearly explain the decisions you made throughout the process, including your choice of features, parameter settings (e.g., the value of k), and data preprocessing steps.\nAnalysis: Discuss your findings in detail. Reflect on the performance of your model, the influence of different features on predictions, and any challenges you encountered during the analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#question-2-attitude-toward-violent-methods-of-resiatnce",
    "href": "HW1.html#question-2-attitude-toward-violent-methods-of-resiatnce",
    "title": "5  Homework 1",
    "section": "5.2 Question 2: Attitude toward violent methods of resiatnce",
    "text": "5.2 Question 2: Attitude toward violent methods of resiatnce\nIn a paper published in Research & Politics, my co-author and I explore the effects of providing information on individuals’ support for nonviolent resistance through an experimental survey. You can access the draft of the paper here.\nFor this assignment, we will use a modified version of the dataset collected for this study. The dataset is publicly available on my GitHub page and can be accessed here.\n\n\n5.2.1 Dataset Description\nBefore assigning participants in the experiment to control and treatment groups, we collected data on several socio-economic factors and participants’ pre-experiment attitudes toward both nonviolent and violent methods of resistance.\nFor this assignment, we will focus on analyzing the variable Violent_method, which measures participants’ attitudes toward violent methods of resistance. Let’s begin by loading the dataset and performing some descriptive analysis.\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Load the dataset\ndata_cl &lt;- read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/LeidenUniv_MAQM2020/Datasets/R%26P_RezaeeAsadzadeh_MethodsCourse.csv\")\n\n# Basic Descriptive Statistics\nnrow(data_cl)                        # Number of observations\n\n[1] 436\n\nsummary(data_cl$Violent_method)      # Summary statistics for Violent_method\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.900   4.081   5.850  10.000       1 \n\n# Kernel Density Plot for 'Violent_method'\nggplot(data_cl, aes(x = Violent_method)) +\n  geom_density(kernel = \"gaussian\", fill = \"darkred\", alpha = 0.6, adjust = 1) +\n  labs(\n    title = \"Kernel Density Estimation (KDE) of Violent Method\",\n    x = \"Violent Method\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.2.2 Assignment: Predicting Attitudes Using k-NN\n\nModel Development:\nUse at least three features from the dataset to build your predictive model. Ensure your k-NN algorithm is cross-validated to assess its performance effectively.\nDocumentation:\nClearly explain the decisions you made throughout the process, including your choice of features, parameter settings (e.g., the value of k), and data preprocessing steps.\nAnalysis: Discuss your findings in detail. Reflect on the performance of your model, the influence of different features on predictions, and any challenges you encountered during the analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "HW1.html#question-3-training-a-naive-bayes-algorithm",
    "href": "HW1.html#question-3-training-a-naive-bayes-algorithm",
    "title": "5  Homework 1",
    "section": "5.3 Question 3: Training a Naive Bayes Algorithm",
    "text": "5.3 Question 3: Training a Naive Bayes Algorithm\nFor this assignment, you will use the annotated data from the article by Antypas, Preece , and Camacho-Collados, pubished in Online Social Networks and Media journal: Link to the article. The replication materials of this study is available via its GitHub repository: Here.\nUse the method that you learned in class to train a Naive Bayes algorithm that can classify the data to positive sentiment and others. The original data annotated the Tweets to Positive, Negative, Neutral, and Indeterminate. For the sake of simplicity, transform/re-code the annotated tweets to positive if +1 and others if other values, such as 0, -1. We do this for the sake of simolicity, and we get back to this later in the class. You can access the raw data via the below link: https://raw.githubusercontent.com/cardiffnlp/politics-and-virality-twitter/refs/heads/main/data/annotation/en/en_900.csv.\nMake sure that your trained Naive Bayes is cross-validated and also report the confusion matrix and (mis)classification rate. Conclude your answer with a brief discussion of the results and the performance of the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#naive-bayes-and-missclassification",
    "href": "NaiveBayes.html#naive-bayes-and-missclassification",
    "title": "3  Naive Bayes",
    "section": "3.7 Naive Bayes and Missclassification",
    "text": "3.7 Naive Bayes and Missclassification\nNow, we are finally ready to do Naive Bayes classification using \\({\\tt e1071}\\) \\(\\mathcal{R}\\) package.\n\nlibrary(e1071)\nsmsNB=naiveBayes(smsTrain,smsTrainY, laplace=1)\nyhat=predict(smsNB,smsTest)\nctab=table(yhat,smsTestY)\nctab\n\n      smsTestY\nyhat    ham spam\n  ham  1333   19\n  spam   19  188\n\nmissClass=1-(sum(ctab)-sum(diag(ctab)))/sum(ctab)\n\nperSpam=ctab[2,2]/sum(ctab[,2])\n\ncat(\"Missclassification and Spam classification rate\",round(missClass,2),\"% and \",round(perSpam,2),\"%, respectively \\n\")\n\nMissclassification and Spam classification rate 0.98 % and  0.91 %, respectively",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#caret-package-a-streamlined-appraoch-to-training-algorithms",
    "href": "NaiveBayes.html#caret-package-a-streamlined-appraoch-to-training-algorithms",
    "title": "3  Naive Bayes",
    "section": "3.8 caret package: a streamlined appraoch to training algorithms",
    "text": "3.8 caret package: a streamlined appraoch to training algorithms",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "NaiveBayes.html#the-caret-package-a-streamlined-approach-to-training-algorithms",
    "href": "NaiveBayes.html#the-caret-package-a-streamlined-approach-to-training-algorithms",
    "title": "3  Naive Bayes",
    "section": "3.8 The caret Package: A Streamlined Approach to Training Algorithms",
    "text": "3.8 The caret Package: A Streamlined Approach to Training Algorithms\nThe caret (Classification and Regression Training) package is one of the most widely used R packages for training machine learning algorithms. It simplifies the process by providing a unified framework for exploring, training, and validating predictive models across a wide range of algorithms.\nIn the previous section, our trained algorithm was not cross-validated, meaning its parameters were not optimized for the highest out-of-sample prediction power. By using caret, we can systematically tune our model to improve its generalization performance.\n\n3.8.1 Instructions\n\nCarefully review the complete code below to get an overall understanding of its workflow.\nRun the code line by line in R.\nFor the in-class assignment, submit an annotated R script where you explain what each line of code does.\n\nUse inline comments (preceded by #) to provide explanations.\nExample:\n\n\n\nprint('Hello, students!')  # This line prints a text message to the console.\n\n[1] \"Hello, students!\"\n\n\nNow, here is the code for training a cross-validated Naïve Bayes algorithm using the same Ham/Spam data set.\n\n## Load required libraries\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(e1071)\nlibrary(caret)\nlibrary(pander) # to get nice looking tables\n\n# a function for % freq tables\nfrqtab &lt;- function(x, caption) {\n  round(100*prop.table(table(x)), 1)\n}\n\n## Load dataset\nsmsData &lt;- read.csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/sms_spam.csv\", \n                    stringsAsFactors = FALSE, encoding=\"UTF-8\")\ndim(smsData)\nsmsData &lt;- smsData[1:999, ]\n\n## Convert label column to factor\nsmsData$type &lt;- factor(smsData$type)\n\n## Text Preprocessing\nsmsCorpus &lt;- VCorpus(VectorSource(smsData$text))\n\nsmsCl &lt;- tm_map(smsCorpus, content_transformer(tolower)) # Convert to lowercase\nsmsCl &lt;- tm_map(smsCl, removeNumbers)  # Remove numbers\nsmsCl &lt;- tm_map(smsCl, removeWords, stopwords()) # Remove stopwords\nsmsCl &lt;- tm_map(smsCl, removePunctuation) # Remove punctuation\nsmsCl &lt;- tm_map(smsCl, stemDocument) # Stemming\nsmsCl &lt;- tm_map(smsCl, stripWhitespace) # Remove extra whitespace\nsmsCl &lt;- tm_map(smsCl, content_transformer(function(x) iconv(x, from = \"UTF-8\", to = \"ASCII\", sub=\"\")))\n\n\n## Create Document-Term Matrix (DTM)\nsmsDTM &lt;- DocumentTermMatrix(smsCl)\n\n## Split data into Training (80%) and Testing (20%)\nset.seed(7)  # For reproducibility\ntrain_index &lt;- createDataPartition(smsData$type, p=0.8, list=FALSE)\n\nsmsData_train &lt;- smsData[train_index,]\nsmsData_test &lt;- smsData[-train_index,]\nsmsCl_train &lt;- smsCl[train_index]\nsmsCl_test &lt;- smsCl[-train_index]\nsmsDTM_train &lt;- smsDTM[train_index,]\nsmsDTM_test &lt;- smsDTM[-train_index,]\n\nft_orig &lt;- frqtab(smsData$type)\nft_train &lt;- frqtab(smsData_train$type)\nft_test &lt;- frqtab(smsData_test$type)\nft_df &lt;- as.data.frame(cbind(ft_orig, ft_train, ft_test))\ncolnames(ft_df) &lt;- c(\"Original\", \"Training set\", \"Test set\")\npander(ft_df, style=\"rmarkdown\",\n       caption=paste0(\"Comparison of SMS type frequencies among datasets\"))\n\n\n# now let's do the final cleanings\n\nsms_dict &lt;- findFreqTerms(smsDTM_train, lowfreq=10)\nsms_train &lt;- DocumentTermMatrix(smsCl_train, list(dictionary=sms_dict))\nsms_test &lt;- DocumentTermMatrix(smsCl_test, list(dictionary=sms_dict))\n\n# modified sligtly fron the code in the book\nconvert_counts &lt;- function(x) {\n  x &lt;- ifelse(x &gt; 0, 1, 0)\n  x &lt;- factor(x, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n}\n\n\nsms_train=apply(sms_train, MARGIN = 2, convert_counts)\nsms_test=apply(sms_test,MARGIN = 2, convert_counts)\n\n\n## Train Naïve Bayes Model with Cross-Validation (5-fold)\ntrain_control &lt;- trainControl(method = \"repeatedcv\",\n                              number = 5,\n                              repeats = 1,\n                              verboseIter = TRUE  # Show training progress\n                              )\nset.seed(7)\nsmsNB_1 &lt;- train(sms_train, smsData_train$type, method=\"nb\",\n                    trControl=train_control)\n\nprint(smsNB_1)\n\nyhat &lt;- predict(smsNB_1, sms_test)\n\n## Confusion Matrix\nctab &lt;- table(yhat, smsData_test$type)\nprint(ctab)\n\n## Calculate Misclassification Rate & Spam Detection Rate\nmissClass &lt;- 1 - sum(diag(ctab)) / sum(ctab)\nperSpam &lt;- ctab[\"spam\", \"spam\"] / sum(ctab[, \"spam\"])\n\ncat(\"Missclassification rate:\", round(missClass, 2) * 100, \"%\\n\")\ncat(\"Spam classification rate:\", round(perSpam, 2) * 100, \"%\\n\")\n\n###\n\ntune_grid &lt;- expand.grid(\n  fL = 1,  # Different Laplace smoothing values\n  usekernel = FALSE,\n  adjust = 1\n)\n\nset.seed(7)\nsmsNB_2 &lt;- train(sms_train, smsData_train$type, method=\"nb\", \n                    tuneGrid=tune_grid,\n                    trControl=train_control)\nprint(smsNB_2)\n\n\nyhat &lt;- predict(smsNB_2, sms_test)\n\n## Confusion Matrix\nctab &lt;- table(yhat, smsData_test$type)\nprint(ctab)\n\n## Calculate Misclassification Rate & Spam Detection Rate\nmissClass &lt;- 1 - sum(diag(ctab)) / sum(ctab)\nperSpam &lt;- ctab[\"spam\", \"spam\"] / sum(ctab[, \"spam\"])\n\ncat(\"Missclassification rate:\", round(missClass, 2) * 100, \"%\\n\")\ncat(\"Spam classification rate:\", round(perSpam, 2) * 100, \"%\\n\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "LLM.html",
    "href": "LLM.html",
    "title": "4  Large Language Model (LLM)",
    "section": "",
    "text": "4.1 Applications in Social Science Research\nRather than replacing established computational social science methods, LLMs serve as a complementary tool that can enhance efficiency, scalability, and analytical depth. Their power lies in assisting with complex text-based tasks, but researchers must remain critical users, ensuring methodological rigor and ethical responsibility in their application.\nBelow, you see how LLM can be used for processing the data using Chat gpt API, but you can use the similar structure with some modfications for using other LLM models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large Language Model (LLM)</span>"
    ]
  },
  {
    "objectID": "LLM.html#applications-in-social-science-research",
    "href": "LLM.html#applications-in-social-science-research",
    "title": "4  Large Language Model (LLM)",
    "section": "",
    "text": "Text Analysis and Content Classification\n\nLLMs can assist in automated text coding, reducing the time needed for manual classification of large-scale datasets.\nExample: Instead of manually labeling social media data, an LLM can be fine-tuned or prompted to identify incivility or political polarization in tweets.\n\nExpanding Computational Approaches\n\nTraditional text analysis methods, such as Naïve Bayes or LDA, rely on structured word counts and frequency distributions.\nLLMs, in contrast, can provide context-aware embeddings, improving sentiment analysis and discourse analysis in complex, nuanced datasets.\n\nStudying Digital Political Behavior\n\nLLMs are particularly useful for analyzing how political actors engage online—whether through social movements, state repression, or digital incivility.\nExample: A model can be used to detect gendered harassment or anti-government discourse in protest-related conversations, helping map how activists and political elites interact online.\n\nEnhancing Research Replicability\n\nBy integrating LLMs into computational workflows, researchers can ensure that their data coding and text classification processes remain consistent and replicable.\nExample: Instead of subjective manual annotation, LLMs can be used to create structured, reproducible content analysis pipelines.\n\nEthical Considerations and Limitations\n\nWhile LLMs offer efficiency, researchers must be critical of biases embedded in these models, especially when analyzing political discourse, misinformation, or contentious movements.\nTransparency in LLM-assisted research is crucial—documenting AI’s role in research workflows is now an essential part of academic integrity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large Language Model (LLM)</span>"
    ]
  },
  {
    "objectID": "LLM.html#an-example-of-text-analysis-using-llm",
    "href": "LLM.html#an-example-of-text-analysis-using-llm",
    "title": "4  Large Language Model (LLM)",
    "section": "4.2 An example of text analysis using LLM",
    "text": "4.2 An example of text analysis using LLM\nWe need to send our request(s) to chat gpt and receive a response. Let’s first send a simple request via and then we see how it can be applied for analyzing text.\n\nlibrary(httr)\nlibrary(stringr)\n\nsend_to_gpt &lt;- function(text, api_key, model = \"gpt-4o-mini\") {\n  url &lt;- \"https://api.openai.com/v1/chat/completions\"\n\n  # Send API request\n  response &lt;- POST(\n    url = url,\n    add_headers(Authorization = paste(\"Bearer\", api_key)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,  # Use \"gpt-4o-mini\" (or change to \"gpt-4o\")\n      messages = list(list(role = \"user\", content = text))\n    )\n  )\n\n  # Parse API response\n  parsed_response &lt;- content(response)\n\n  # Extract message safely\n  if (!is.null(parsed_response$choices) && length(parsed_response$choices) &gt; 0) {\n    return(parsed_response$choices[[1]]$message$content)\n  } else {\n    print(\"Error: No valid response from API.\")\n    return(NULL)\n  }\n}\n\nNow that we wrote the function for sending and reciving message from chat gpt API, now we can communicate with it\nAs you can see, the API returns a variety of information, including metadata about the request and the model’s response.\nNow, let’s use the API for text analysis. We will ask ChatGPT to analyze the latest tweet by NATO’s Secretary General, Mark Rutte. Specifically, we want to extract:\nSentiment (positive, neutral, or negative) Tone (e.g., formal, urgent, diplomatic, critical) To keep the output focused, we will limit the printed response to only the relevant content—the model’s analysis of the tweet.\nCan you see how this is done? Let’s try it!\n\ntext_input &lt;- \"Read the below tweet by a politician. Please analyze the tweet and return the sentiment (positive, negative, or neutral), and tone of the message. Make sure that the output Format is: \\n 'Sentiment: [sentiment] ; Tone: [tone]'. Here is the tweet: Ready and willing. That’s my take from today’s meeting in Paris. Europe is ready and willing to step up. To lead in providing security guarantees for Ukraine. Ready and willing to invest a lot more in our security. The details will need  to be decided but the commitment is clear. \"\n\nresponse_text &lt;- send_to_gpt(text_input, api_key)\n\n[1] \"Error: No valid response from API.\"\n\nprint(response_text)\n\nNULL",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large Language Model (LLM)</span>"
    ]
  },
  {
    "objectID": "LLM.html#analyzing-the-sentiment-of-posts-by-nato-twitter-account",
    "href": "LLM.html#analyzing-the-sentiment-of-posts-by-nato-twitter-account",
    "title": "4  Large Language Model (LLM)",
    "section": "4.3 Analyzing the Sentiment of Posts by NATO Twitter Account",
    "text": "4.3 Analyzing the Sentiment of Posts by NATO Twitter Account\nIn this section, we will apply ChatGPT’s API to analyze the sentiment and tone of NATO’s latest tweets. The objective is to process multiple observations from a dataset, classify their sentiment and tone, and store the results in a CSV file for further analysis.\nWe will proceed with the following steps:\n\nDefine an API function to communicate with ChatGPT.\nLoad the dataset of NATO tweets.\nCreate a function to extract sentiment and tone from each tweet.\nIterate through the dataset, apply the function, and store results.\nSave the enriched dataset with sentiment and tone classifications.\n\n\n\n4.3.1 Step 1: Defining a Function to Connect to ChatGPT API\nFirst, we need to define a function to send a text prompt to ChatGPT and retrieve a response.\nThis function will act as an interface between our dataset and the OpenAI API.\n\nlibrary(httr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(progress) \n\nsend_to_gpt &lt;- function(text, api_key, model = \"gpt-4o-mini\") {\n  \n  url &lt;- \"https://api.openai.com/v1/chat/completions\"\n  \n  # Send API request\n  response &lt;- POST(\n    url = url,\n    add_headers(Authorization = paste(\"Bearer\", api_key)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,  # Use \"gpt-4o-mini\" (or change to \"gpt-4o\")\n      messages = list(list(role = \"user\", content = text))\n    )\n  )\n  \n  # Parse API response\n  parsed_response &lt;- content(response, as = \"parsed\")\n  \n  # Extract message safely\n  if (!is.null(parsed_response$choices) && length(parsed_response$choices) &gt; 0) {\n    return(parsed_response$choices[[1]]$message$content)\n  } else {\n    print(\"Error: No valid response from API.\")\n    return(NULL)\n  }\n}\n\n\n\n4.3.2 Step 2: Load the NATO Tweets Dataset\nNext, we will import a dataset of NATO’s tweets from a CSV file. This dataset contains recent tweets from NATO’s official Twitter/X account.\n\n# Load CSV file containing NATO tweets\nnato_tweets &lt;- read_csv(\"https://raw.githubusercontent.com/babakrezaee/MethodsCourses/refs/heads/master/DataSets/nato_tweets.csv\")\n\n# Check structure of dataset\nhead(nato_tweets)\n\n# A tibble: 6 × 2\n  created   tweet_text                                                          \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 18-2-2025 \"NATO's largest exercise in 2025 is underway\\n\\n10.000 troops from …\n2 18-2-2025 \"Ready and willing. That’s my take from today’s meeting in Paris. E…\n3 17-2-2025 \"Honoured to be part of the opening of #NATO’s Joint Analysis, Trai…\n4 17-2-2025 \"\\\"@DepSecGenNATO\\n travelled to \\U0001f1f5\\U0001f1f1 Poland to par…\n5 17-2-2025 \"Today, \\n@SecGenNATO\\n Mark Rutte welcomed \\n@SPE_Kellogg\\n, Unite…\n6 17-2-2025 \"So good to host \\n@SPE_Kellogg\\n in Brussels. Today he consulted w…\n\n# Display column names\ncolnames(nato_tweets)\n\n[1] \"created\"    \"tweet_text\"\n\n# Count number of tweets\nnrow(nato_tweets)\n\n[1] 22\n\n\n\n\n4.3.3 Step 3: Writing a Function for Tweet Analysis\nNow, we define a function that:\n\nSends each tweet to ChatGPT.\nExtracts the sentiment and tone from the response.\n\n\n# Define API Key (Ensure you use your API key, as this one will be disabled soon!)\napi_key &lt;- \"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\"\n\n# Function to get sentiment & tone from GPT\nanalyze_tweet &lt;- function(tweet) {\n  prompt &lt;- paste(\n    \"Analyze the following tweet's sentiment and tone.\",\n    \"Classify the sentiment as positive, neutral, or negative.\",\n    \"Classify the tone as formal, urgent, diplomatic, critical, or neutral.\",\n    \"Provide the response in the following format:\",\n    \"'Sentiment: [sentiment]; Tone: [tone]'\",\n    \"\\n\\nTweet:\", tweet\n  )\n  \n  response &lt;- send_to_gpt(prompt, api_key)\n  \n  if (!is.null(response)) {\n    # Extract sentiment and tone using regex\n    sentiment_match &lt;- str_extract(response, \"Sentiment:\\\\s*(\\\\w+)\")\n    tone_match &lt;- str_extract(response, \"Tone:\\\\s*(\\\\w+)\")\n    \n    sentiment &lt;- ifelse(!is.na(sentiment_match), gsub(\"Sentiment:\\\\s*\", \"\", sentiment_match), NA)\n    tone &lt;- ifelse(!is.na(tone_match), gsub(\"Tone:\\\\s*\", \"\", tone_match), NA)\n    \n    return(c(sentiment, tone))\n  } else {\n    return(c(NA, NA))\n  }\n}\n\n\n\n4.3.4 Step 4: Analyzing the Dataset & Storing Results\nNow, we iterate over the dataset and:\n\nSend each tweet to ChatGPT for analysis.\nStore the extracted sentiment and tone in a new dataframe.\nUse a progress bar to monitor the API request process.\n\n\n# Define an empty matrix to store results\nresults &lt;- matrix(NA, nrow = nrow(nato_tweets), ncol = 2)\n\n# Set column names\ncolnames(results) &lt;- c(\"sentiment\", \"tone\")\n\n# Initialize progress bar\npb &lt;- txtProgressBar(min = 0, max = nrow(nato_tweets), style = 3)\n\n\n  |                                                                            \n  |                                                                      |   0%\n\n# Loop through each tweet\nfor (i in 1:nrow(nato_tweets)) {\n  results[i, ] &lt;- analyze_tweet(nato_tweets$tweet_text[i])  # Analyze tweet and store result\n  \n  setTxtProgressBar(pb, i)  # Update progress bar\n}\n\n[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |===                                                                   |   5%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |======                                                                |   9%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |==========                                                            |  14%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=============                                                         |  18%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |================                                                      |  23%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |===================                                                   |  27%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |======================                                                |  32%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=========================                                             |  36%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=============================                                         |  41%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |================================                                      |  45%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |===================================                                   |  50%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |======================================                                |  55%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=========================================                             |  59%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=============================================                         |  64%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |================================================                      |  68%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |===================================================                   |  73%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |======================================================                |  77%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |=========================================================             |  82%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |============================================================          |  86%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |================================================================      |  91%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |===================================================================   |  95%[1] \"Error: No valid response from API.\"\n\n  |                                                                            \n  |======================================================================| 100%\n\nclose(pb)  # Close progress bar\n\n\n\n4.3.5 Step 5: Saving the Final Results\nOnce the analysis is complete, we:\n\nConvert results into a dataframe.\nMerge them with the original dataset.\nSave everything as a new CSV file.\n\n\n# Convert results to a dataframe\nresults_df &lt;- as.data.frame(results, stringsAsFactors = FALSE)\ncolnames(results_df) &lt;- c(\"sentiment\", \"tone\")\n\n# Merge results with original dataset\nnato_tweets &lt;- cbind(nato_tweets, results_df)\n\n# Save to CSV\nwrite_csv(nato_tweets, \"nato_tweets_with_sentiment.csv\")\n\n# View results\nhead(nato_tweets)\n\n    created\n1 18-2-2025\n2 18-2-2025\n3 17-2-2025\n4 17-2-2025\n5 17-2-2025\n6 17-2-2025\n                                                                                                                                                                                                                                                                                                  tweet_text\n1 NATO's largest exercise in 2025 is underway\\n\\n10.000 troops from 9 Allies are training in Romania 🇷🇴 and Bulgaria 🇧🇬 \\n\\nThe exercise is based on NATO's new defence plans and will focus on the planning and execution of a pre-crisis multi-domain scenario 💪\\n\\n🔗https://shape.nato.int/steadfast-dart\n2                   Ready and willing. That’s my take from today’s meeting in Paris. Europe is ready and willing to step up. To lead in providing security guarantees for Ukraine. Ready and willing to invest a lot more in our security. The details will need  to be decided but the commitment is clear.\n3                                    Honoured to be part of the opening of #NATO’s Joint Analysis, Training and Education Centre in Bydgoszcz #Poland.\\nJATEC is part of NATO’s enduring commitment to #Ukraine 🇺🇦. It will only further strengthen our deterrence and defence making us stronger and safer.\n4                    \"@DepSecGenNATO\\n travelled to 🇵🇱 Poland to participate in the opening ceremony of the NATO Joint Analysis, Training and Education Centre. \\n\\nSpeaking in the ceremony, the Deputy Secretary General said that the Centre’s work “will help the Alliance and Ukraine become stronger.”\n5                                                        Today, \\n@SecGenNATO\\n Mark Rutte welcomed \\n@SPE_Kellogg\\n, United States Special Envoy for Ukraine and Russia, to the North Atlantic Council for a discussion with Allies on ending the war against Ukraine\\n\\nTap for more info on the meeting ↓\n6                                         So good to host \\n@SPE_Kellogg\\n in Brussels. Today he consulted with Allies at NATO on working together to ensure just and lasting peace in Ukraine. We have a lot to do and will cooperate closely to pursue an end to the war and enduring security for us all.\n  sentiment tone\n1        NA   NA\n2        NA   NA\n3        NA   NA\n4        NA   NA\n5        NA   NA\n6        NA   NA\n\n\n\n\n4.3.6 In-class assignment\nIn the Naive Bayes session, you worked with the Spam/Ham SMS dataset, where you built a classifier to detect spam messages. Today, you will use ChatGPT API to classify a subset of this dataset (30 observations) and evaluate its effectiveness.\nYour assignment is to:\n\nLimit the dataset to 30 observations.\nSend each SMS message to ChatGPT and ask it to classify it as “Spam” or “Ham”.\nCompare ChatGPT’s classifications with the actual dataset labels.\n\nAnalyze ChatGPT’s accuracy and discuss its performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large Language Model (LLM)</span>"
    ]
  }
]