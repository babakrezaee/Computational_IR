---
title: "Tree Models: Random Forest"
format: html
---

```{r setup, include=FALSE}
install.packages("latex2exp")  # Only needed once
library(latex2exp)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
#suppressPackageStartupMessages(library(KKNN))
#suppressPackageStartupMessages(library(MASS))

```


The idea of trees in statistical learning has similarity with decision trees models. Trees and its extensions have been used widely for modeling the prediction of outcome variables. Let's start with reviewing the advantages and disadvantages of this method:

*Benefits:*

* Very good in capturing the interactions between the features of a models and non-linearity associations

* performs very well in handling categorical variables, with nice interpretation of the results

* relatively faster than other machine learning algorithms

* does not require scaling x variables/features.


*Disadvantage:*


* Simple tree models do not lead to the best out-of-sample predictive. 

However, if we grow a good forest and bag/boost the models to combine the fits from different tree models, we can improve the prediction power. 

## Regression Trees

It is much easier to explain the tree models using an example. Let's start with the Boston housing data that we used before in $kNN$ section. Similarly, I want to predict median housing price in a neighborhood, ${\tt medv}$, using the ratio of low state households, ${\tt lstat}$.

```{r}
library(MASS) 
attach(Boston)
n = nrow(Boston)

ddf = data.frame(lstat,medv)

# Sort the data here
oo=order(ddf$lstat)

ddf = ddf[oo,]


library(rpart)

m_tree=rpart(medv~lstat,data=ddf)

#plot the estimated tree

summary(m_tree)

summary(ddf$medv)

plot(m_tree)

text(m_tree)
```

But, this plot does not often look good. Indeed, it usually is a mess! To plot a nice tree, I suggest using ${\tt rpart.plot}$ package.


```{r}

library(rpart.plot)

rpart.plot(m_tree)


```

At each interior node, there is decision rule, $x>c$. If $x>c$, you should follow the branch on the left. This continues until you reach a bottom/terminal node, which also is known as the leaf of the tree.

```{r}

plot(ddf$lstat,ddf$medv, col='blue')

lines(ddf$lstat,predict(m_tree,ddf),col='maroon',lwd=3)


for (i in m_tree$splits[,4] ){
abline(v=i, col="orange",lty=2)
}

```


The set of bottom nodes gives us a partition of the predictor $x$ space into disjoint regions. At right, the vertical lines display the
partition. With just one x, this is just a set of intervals.

Within each region (interval) we compute the average of the $y$ values for the subset of training data in the region. This gives us the step function which is our $\hat{f}$. The $\bar{y}$ values are also printed at the bottom nodes. To predict, we just use the above step function estimation of f(x).


## A Tree with Two Explanatory Variables


Now, let's estimate a tree model using two features: $x=(lstat, dis)$ and $y=medx$.

First, let's take a look at scatter plot of these variables:

```{r}

attach(Boston)

ddf = data.frame(medv,rm,nox,lstat, dis)

# Sort the data here
oo=order(ddf$lstat)

ddf = ddf[oo,]

library(ggplot2)

ggplot(ddf, aes(lstat, dis, colour = medv)) + geom_point()

```


```{r}


m_tree2=rpart(medv~lstat+dis,data=ddf)


rpart.plot(m_tree2, box.palette = "Grays")

print(m_tree2)

summary(m_tree2)

```

```{r}

library(tree)

m_tree2=tree(medv~lstat+dis,data=ddf)

summary(m_tree2)


partition.tree(m_tree2)


```



```{r}

attach(Boston)
mid<-mean(medv)
ggplot(ddf, aes(lstat, rm, colour = medv)) + geom_point()+scale_color_gradient2(midpoint=mid, low="blue", mid="cyan",
                     high="maroon", space ="Lab" )



m_tree31=rpart(medv~lstat+rm+dis,data=ddf)

summary(m_tree31)

rpart.plot(m_tree31, box.palette = "Grays")



```

```{r}
m_tree32=rpart(medv~lstat+rm+dis,data=ddf)


rpart.plot(m_tree32, box.palette = "Grays")

```

## What is the loss problem and optimization question in tree algorithm?

As shown above, we want to split the feature space, i.e. $X=\{x_1, x_2, \dots, x_k\}$, to smaller rectangular, or boxes. If we find a splitting pattern which has the lowest prediction error for the training data, then we can predict the associated outcome with the prediction feature set $X_p$ by finding to which box this set belongs. Roughly, we can think about about tree algorithm like a $kNN$ algorithm in which $k$ is dynamic and can change according to the changes in the data.

\begin{align}

min_{R_1,R_2,\dots,R_J} \sum_{j=1}^J \sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2

\end{align}

This loss has a straightforward intuition: we want to find $R_j$s to minimize the sum of the squared deviation of observations from the mean response within each box. In theory, we can define infinite number of boxes, and we need to evaluate the squared errors of each set of boxes to see which one is minimized. This is computationally very intensive and almost impossible. 

To solve this problem, a *recursive partitioning* top-down approach is used to find the *optimal* boxes:

* Scan each feature/predictor in the data to find the one for which splitting gives the best performance/prediction power, i.e. lowest error.

* Now, for each split repeat the previous step, and continue this until splitting is too costly (we will discuss later what "costly" means, but just as a tip: *think about bias-variance tradeoff*), or reach a pre-defined tree size.

This is a *greedy* approach since the splitting process only focus on each step and pick the split that is associated with the best fit at the current step, rather than looking ahead and picking the split which lead to a better prediction tree in the following steps.

## How to fit the data to a tree model?

To execute the above algorithm, statistical algorithms re-write the above problem as follow:

\begin{align}

C(T,y)=L(T,y)+\alpha |T|

\end{align}

where L(T,y) is the loss of fitting outcome $y$ with tree $T$. Our goal always is minimizing the loss, that is small $L$ is preferred. 

However, we do not want to make the model/tree too complex. A complex model might be good in capturing all variances in the training data, but would lead to a biased prediction of in the test data. Thus, we add the number of nodes in tree T, i.e. $|T|$, with a penalty parameter $\alpha$ to the cost function $C(T,y)$. For the continuous $y$, we can use *RMSE*, and for categorical $y$, we can use any of the classification measures that are discussed in the previous section.

What is the best $\alpha$? if we pick a big $\alpha$, then the penalty for having a big and complex tree is large. Thus, the optimization problem return a small tree which can lead to a large $L$ on the training data. On the other hand, for a small $\alpha$, we allow the model pick a big tree. Here, $\alpha$ is analogous to $k$ in $kNN$. How do we pick $\alpha$? The answer similar to most of the similar cases in this course is *cross-validation*.


## Pruning trees


The idea here is first allow a tree grows as much as possible without being worried about its complexity size. Then, considering the complexity parameter, CP value, we prune/cut the tree with the optimal CP value. 


```{r}
attach(Boston)

df=Boston[,c(8,13,14)] # pick dis, lstat, medv

print(names(df))


# First grow a big tree

tree_ml=tree(medv~., df)

print(tree_ml)

plot(tree_ml)
text(tree_ml)

cat("Size of the big tree: \n")
print(length(unique(tree_ml$where)))

Boston_fit=predict(tree_ml,df)

# Now, let's prune it down to a tree with 6 leaves/nodes

tree_ml_7=prune.tree(tree_ml,best=7)

cat("Size of the big tree: \n")
print(length(unique(tree_ml_7$where)))


par(mfrow=c(1,2))

plot(tree_ml_7, type='u')
text(tree_ml_7, col='maroon', label=c('yval'), cex=.9)


Boston_fit7=predict(tree_ml_7,df)

##

ResultsMat=cbind(medv,Boston_fit,Boston_fit7)

colnames(ResultsMat)=c("medv","tree","tree7")

pairs(ResultsMat, col='maroon')

print(cor(ResultsMat))

#Let's use the trained algorithm to predict the value for a specific point

Pred_point=data.frame(lstat=15,dis=2)

yhat=predict(tree_ml,Pred_point)

cat('prediction is: \n')
print(yhat)

```

## A cross-validated problem

```{r}

attach(Boston)
set.seed(7)

df=Boston[,c(8,13,14)]

print(names(df))

# Let's fit a single tree and plot the importance of the variables

tree=rpart(medv~., method="anova",data=df,
           control=rpart.control(minsplit = 5,cp=.0005))


Ntree=length(unique(tree$where))

cat("Size of big tree is:",Ntree,"\n")

# Let's check the CV results

plotcp(tree)

best_a=which.min(tree$cptable[,"xerror"])

bestCP=tree$cptable[best_a,"CP"]

bestSize=tree$cptable[best_a,"nsplit"]+1

best_tree=prune(tree,cp=bestCP)

plot(best_tree,uniform=TRUE)
text(best_tree,digits=2, use.n = TRUE)

```

## Bagging and Random Forest

The idea behind *Bagging* is bootstrapping the data adequately enough to make sure that trees with good explanatory power are captured. This approach is computationally intensive and time-consuming.

Random Forests starts from *Bagging* and adds another kind of randomization. In this method, instead of going over all features when we do the greedy approach, this method randomly samples a subset of $m$ variables to search over each time we make a split.

In this method, more types of trees will be evaluated. Since this method is Bootstraped, the important variables will be identified in average more than others; and thus can be used for prediction.

How to choose parameters for a Random Forest model? $B$ is the number of Bootstrapped samples, and $m$ is the number of variables to sample. A common choice for $m$ is $\sqrt p$, where $p$ is the number of features in the model. When we set $m=p$, then Random Forest is Bagging.

## California Housing Data 


```{r}
library(randomForest)


rawData=read.csv("https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/calhouse.csv")

# First divide the sample to train, validation, and test samples


set.seed(7)


n=nrow(rawData)

n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2

#Shuffle the data
ii=sample(1:n,n)

CAtrain=rawData[ii[1:n1],]
CAval=rawData[ii[n1+1:n2],]
CAtest=rawData[ii[n1+n2+1:n3],]

## Fitting using the RF on train and evaluate on the tes, and predict on val

RF=randomForest(logMedVal~.,data=CAtrain,mtry=3,ntree=500)

RFpred=predict(RF,newdata=CAval)

RMSE=sqrt(mean(CAval$logMedVal-RFpred)^2)

cat('RMSE on the train data for Random Forest is',RMSE,"\n")


getTree(RF, 1, labelVar=TRUE)[1:30,]



pairs(cbind(CAval$logMedVal,RFpred))

print(cor(cbind(CAval$logMedVal,RFpred)))

varImpPlot(RF)


```

